### `backend\app.py`

```python
import os
import json
import csv
import logging
from decimal import Decimal
from datetime import datetime
from functools import wraps

import numpy as np
import pandas as pd
import joblib
import tensorflow as tf 
from tensorflow.keras.models import load_model 

from flask import Flask, jsonify, request
from flask_cors import CORS, cross_origin
from flask_bcrypt import Bcrypt
from flask_jwt_extended import (
    JWTManager, create_access_token,
    jwt_required, get_jwt_identity, get_jwt
)
from werkzeug.exceptions import HTTPException
from sqlalchemy import or_, func

from database import db
from models import Role, User, Vehicle, CartItem, Order, OrderItem, Review


app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "http://localhost:5173"}}, supports_credentials=True)

app.logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
app.logger.handlers = [handler] 

app.logger.info("Flask application logger configured.")

# config
app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get(
    'DATABASE_URL',
    'mysql+pymysql://root:PisyaMorja123@localhost/db_name'
)
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.config['JWT_SECRET_KEY'] = os.environ.get(
    'JWT_SECRET_KEY',
    'your-super-secret-fallback-key-for-admin-panel'
)

db.init_app(app)
bcrypt = Bcrypt(app)
jwt = JWTManager(app)

ARTIFACTS_PATH = os.path.join(os.path.dirname(__file__), 'models', 'recommendation_artifacts_csv.joblib')
if os.path.exists(ARTIFACTS_PATH):
    try:
        art = joblib.load(ARTIFACTS_PATH)
        app.logger.info(f"Loaded CF artifacts from {ARTIFACTS_PATH}")
    except Exception as e:
        app.logger.error(f"Failed loading CF artifacts: {e}", exc_info=True)
        art = {}
else:
    app.logger.error(f"CF artifacts not found at {ARTIFACTS_PATH}")
    art = {}

user_knn = art.get('user_knn')
item_knn = art.get('item_knn')
item_pipeline = art.get('item_pipeline')
train_centered_idx = art.get('train_centered_index')
train_centered_cols = art.get('train_centered_columns')
user_means = art.get('user_means', {})
user_history = art.get('user_rated_history', {})
X_items_index = art.get('X_items_df_index', [])
X_items_used_cat_features = art.get('X_items_df_used_cat_features', [])
X_items_used_num_features = art.get('X_items_df_used_num_features', [])
X_ITEMS_PIPELINE_COLS = X_items_used_cat_features + X_items_used_num_features
TOP_MODELS_FOR_GROUPING = art.get('top_models_for_grouping', [])
global_mean = art.get('global_mean_rating', 3.0)
MIN_RATING = art.get('MIN_RATING', 1.0)
MAX_RATING = art.get('MAX_RATING', 5.0)
best_alpha = art.get('best_alpha_hybrid', 0.5)

app.logger.info(f"Type of user_knn after loading: {type(user_knn)}")


BASE_DIR        = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR      = os.path.join(BASE_DIR, 'models')
SAVED_DIR       = os.path.join(BASE_DIR, 'saved_recommender')

tf_user_ids = []
tf_item_ids = []
item_embeddings_norm = None
user_lookup = None
user_model = None

try:
    with open(os.path.join(MODELS_DIR, 'user_ids.json'), 'r', encoding='utf-8') as f:
        tf_user_ids = json.load(f)
    with open(os.path.join(MODELS_DIR, 'item_ids.json'), 'r', encoding='utf-8') as f:
        tf_item_ids = json.load(f)

    item_embeddings_path = os.path.join(MODELS_DIR, 'item_embeddings.npy')
    if os.path.exists(item_embeddings_path):
        item_embeddings = np.load(item_embeddings_path)
        _norms = np.linalg.norm(item_embeddings, axis=1, keepdims=True)
        item_embeddings_norm = item_embeddings / (_norms + 1e-10)
        app.logger.info("Loaded TF item embeddings.")
    else:
        app.logger.error(f"TF item_embeddings.npy not found at {item_embeddings_path}")


    if tf_user_ids:
        user_lookup   = tf.keras.layers.StringLookup(vocabulary=tf_user_ids, mask_token=None)
        user_model_path = os.path.join(SAVED_DIR, 'user_model.h5')
        if os.path.exists(user_model_path):
            user_model    = load_model(user_model_path, compile=False)
            app.logger.info("Loaded TF user model and lookup.")
        else:
            app.logger.error(f"TF user_model.h5 not found at {user_model_path}")
    else:
        app.logger.error("TF user_ids not loaded, cannot initialize user_lookup and user_model.")

except FileNotFoundError as fnf_error:
    app.logger.error(f"TF Model loading error: File not found - {fnf_error}", exc_info=True)
except Exception as e:
    app.logger.error(f"General error loading TF Model components: {e}", exc_info=True)

# Инициализация БД и загрузка данных
with app.app_context():
    app.logger.info("Application context entered for DB setup.")
    db.create_all()
    app.logger.info("db.create_all() called (creates tables if they don't exist).")

    user_role_created = False
    admin_role_created = False
    if not Role.query.filter_by(name='user').first():
        app.logger.info("Default 'user' role not found, creating it.")
        db.session.add(Role(name='user'))
        user_role_created = True
    if not Role.query.filter_by(name='admin').first():
        app.logger.info("Default 'admin' role not found, creating it.")
        db.session.add(Role(name='admin'))
        admin_role_created = True

    if db.session.new:
        db.session.commit()
        app.logger.info("Default roles committed if they were created.")
    else:
        app.logger.info("Default roles already existed or no new roles were added.")
    if not Vehicle.query.first():
        app.logger.info("No vehicles found in DB, attempting to load from CSV.")
        csv_path = os.path.join(os.path.dirname(__file__), 'data', 'project_vehicle_dataset.csv')
        if os.path.exists(csv_path):
            app.logger.info(f"Vehicles CSV file found at {csv_path}.")
            try:
                with open(csv_path, newline='', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    vehicles_to_add = []
                    required_cols_for_value_check = ['vehicle_id', 'brand', 'body_type', 'year', 'gearbox', 'drive', 'power_hp', 'seats', 'price_per_day_usd']
                    for row_num, row in enumerate(reader):
                        try:
                            if not all(col in row and str(row[col]).strip() for col in required_cols_for_value_check):
                                app.logger.warning(f"App CSV Load: Missing required data in row {row_num+1}. Skipping. Row: {row}")
                                continue
                            vehicle_id_str = str(row['vehicle_id']).strip()
                            image_url_val = str(row.get('image', '')).strip()
                            if not image_url_val or image_url_val.lower() == 'n/a': image_url_val = None
                            model_val = str(row.get('model', '')).strip()
                            if not model_val: model_val = None
                            v = Vehicle(
                                vehicle_id=vehicle_id_str, brand=str(row['brand']).strip(), model=model_val,
                                body_type=str(row['body_type']).strip(), year=int(row['year']),
                                gearbox=str(row['gearbox']).strip(), drive=str(row['drive']).strip(),
                                power_hp=int(row['power_hp']), seats=int(row['seats']),
                                price_per_day_usd=Decimal(row['price_per_day_usd']), image_url=image_url_val
                            )
                            vehicles_to_add.append(v)
                        except KeyError as ke: app.logger.error(f"App CSV Load Error: Missing key {ke} in row {row_num+1}: {row}")
                        except ValueError as ve: app.logger.error(f"App CSV Load Error: Value error {ve} in row {row_num+1}: {row}")
                        except Exception as e_inner: app.logger.error(f"App CSV Load: Generic error in row {row_num+1}: {e_inner}. Row: {row}", exc_info=True)
                    if vehicles_to_add:
                        db.session.bulk_save_objects(vehicles_to_add); db.session.commit()
                        app.logger.info(f"{len(vehicles_to_add)} vehicles loaded from CSV into app DB and committed.")
                    else: app.logger.info("No new vehicles to add from CSV (or all rows had errors) during app startup.")
            except Exception as e:
                app.logger.error(f"Error loading vehicles from CSV during app startup: {e}", exc_info=True)
                db.session.rollback()
        else: app.logger.warning(f"Vehicles CSV file not found at {csv_path} during app startup.")
    else: app.logger.info("Vehicles already exist in the database, skipping CSV load during app startup.")

# Обратные вызоы JWT
@jwt.expired_token_loader
def expired_token_callback(jwt_header, jwt_payload):
    return jsonify(msg="Token has expired", error_code="TOKEN_EXPIRED"), 401

@jwt.invalid_token_loader
def invalid_token_callback(err_string): # param name from target (err) or user (error_string)
    return jsonify(msg=f"Invalid Token: {err_string}", error_code="INVALID_TOKEN"), 422

@jwt.unauthorized_loader
def missing_token_callback(err_string): # param name from target (err) or user (error_string)
    return jsonify(msg=f"Authorization Required: {err_string}", error_code="AUTH_REQUIRED"), 401 # user had AUTHORIZATION_REQUIRED

@jwt.needs_fresh_token_loader
def fresh_token_callback(jwt_header, jwt_data): # user had token_not_fresh_callback
    return jsonify(msg="Fresh token required", error_code="FRESH_TOKEN_REQUIRED"), 401


# Использование пользователя для улучшения обработки журнала
def get_current_user_id():
    jwt_user_id_str = get_jwt_identity()
    try:
        return int(jwt_user_id_str)
    except (ValueError, TypeError):
        app.logger.error(f"Invalid JWT identity: '{jwt_user_id_str}'. Cannot convert to int.")
        raise ValueError(f"Invalid user identity in token: {jwt_user_id_str}")

def admin_required(fn):
    @wraps(fn)
    @jwt_required()
    def wrapper(*args, **kwargs):
        current_user_claims = get_jwt()
        user_role = current_user_claims.get('role')
        if user_role != 'admin':
            user_id_for_log = get_jwt_identity() 
            app.logger.warning(f"Admin access denied for user ID: {user_id_for_log}. Role in token: '{user_role}'")
            return jsonify(msg="Administration rights required."), 403 
        return fn(*args, **kwargs)
    return wrapper

# Функции прогнозирования (пользователя)
def predict_user_cf_revised(user_id, vehicle_id_to_predict_str, current_user_actual_history=None):
    user_specific_mean = user_means.get(user_id, global_mean) if user_means is not None else global_mean
    if user_knn is None or train_centered_idx is None or user_id not in train_centered_idx or train_centered_cols is None:
        return user_specific_mean
    user_ratings_for_train_items = pd.Series(0.0, index=train_centered_cols)
    hist_to_use = current_user_actual_history if current_user_actual_history is not None else user_history.get(user_id, {})
    for item_id_hist_str, rating_hist in hist_to_use.items():
        if item_id_hist_str in train_centered_cols:
            try: user_ratings_for_train_items[item_id_hist_str] = float(rating_hist) - user_specific_mean
            except (ValueError, TypeError): pass
    centered_user_vector_values = user_ratings_for_train_items.values
    if not np.any(centered_user_vector_values): return user_specific_mean
    try:
        num_samples_fit = user_knn.n_samples_fit_
        k_neighbors = min(user_knn.n_neighbors, num_samples_fit -1 if num_samples_fit > 1 else 0)
        if k_neighbors <= 0 : return user_specific_mean
        distances, neighbor_indices_flat = user_knn.kneighbors(centered_user_vector_values.reshape(1, -1), n_neighbors=k_neighbors)
        neighbor_indices_flat = neighbor_indices_flat.flatten()
    except Exception as e:
        app.logger.error(f"UCF: Error in kneighbors for user {user_id}, item '{vehicle_id_to_predict_str}': {e}", exc_info=True)
        return user_specific_mean
    similarities = 1.0 - distances.flatten()
    valid_neighbor_indices = [idx for idx in neighbor_indices_flat if 0 <= idx < len(train_centered_idx)]
    neighbor_user_ids_from_idx = [train_centered_idx[i] for i in valid_neighbor_indices]
    similarities = np.array([sim for i, sim in enumerate(similarities) if neighbor_indices_flat[i] in valid_neighbor_indices])
    numerator = 0.0
    denominator = 0.0
    for sim_score, neighbor_uid in zip(similarities, neighbor_user_ids_from_idx):
        if sim_score <= 1e-6: continue
        neighbor_hist_artifact = user_history.get(neighbor_uid, {})
        neighbor_rating_for_item = neighbor_hist_artifact.get(vehicle_id_to_predict_str)
        if neighbor_rating_for_item is not None:
            try:
                neighbor_rating_val = float(neighbor_rating_for_item)
                neighbor_mean_rating = user_means.get(neighbor_uid, global_mean) if user_means is not None else global_mean
                numerator += sim_score * (neighbor_rating_val - neighbor_mean_rating)
                denominator += sim_score
            except (ValueError, TypeError): pass
    if denominator < 1e-6 : return user_specific_mean
    predicted_rating = user_specific_mean + (numerator / denominator)
    return np.clip(predicted_rating, MIN_RATING, MAX_RATING)

def predict_content_cf_revised(user_id, vehicle_id_to_predict_str, current_user_actual_history=None):
    user_specific_mean = user_means.get(user_id, global_mean) if user_means is not None else global_mean
    if item_knn is None or item_pipeline is None or X_items_index is None or not X_ITEMS_PIPELINE_COLS:
        return user_specific_mean
    target_vehicle = db.session.get(Vehicle, vehicle_id_to_predict_str)
    if not target_vehicle: return user_specific_mean
    item_features_dict = {}
    for feature_name in X_ITEMS_PIPELINE_COLS:
        if feature_name == 'model_grouped':
            raw_model_value = getattr(target_vehicle, 'model', None)
            item_features_dict['model_grouped'] = raw_model_value if raw_model_value and raw_model_value in TOP_MODELS_FOR_GROUPING else 'Other_Model'
        else: item_features_dict[feature_name] = getattr(target_vehicle, feature_name, None)
    try:
        item_features_df = pd.DataFrame([item_features_dict])[X_ITEMS_PIPELINE_COLS]
        transformed_item_vector = item_pipeline.transform(item_features_df)
    except Exception as e:
        app.logger.error(f"CCF: Error in item_pipeline.transform for vehicle '{vehicle_id_to_predict_str}': {e}", exc_info=True)
        app.logger.error(f"CCF: Features passed to pipeline: {item_features_dict}")
        return user_specific_mean
    num_items_for_knn = len(X_items_index) if X_items_index is not None else 0
    desired_k_for_query = (item_knn.n_neighbors if hasattr(item_knn, 'n_neighbors') else 5) + 1
    k_neighbors_param = min(desired_k_for_query, num_items_for_knn)
    if k_neighbors_param <= 0: return user_specific_mean
    try:
        distances, neighbor_indices_flat = item_knn.kneighbors(transformed_item_vector, n_neighbors=k_neighbors_param)
        neighbor_indices_flat = neighbor_indices_flat.flatten()
    except Exception as e:
        app.logger.error(f"CCF: Error in item_knn.kneighbors for vehicle '{vehicle_id_to_predict_str}': {e}", exc_info=True)
        return user_specific_mean
    similar_vehicle_ids_str = []
    for idx in neighbor_indices_flat:
        if 0 <= idx < num_items_for_knn:
            vid_str = X_items_index[idx]
            if vid_str != vehicle_id_to_predict_str: similar_vehicle_ids_str.append(vid_str)
    if not similar_vehicle_ids_str: return user_specific_mean
    user_ratings_for_similar_items = []
    hist_to_use_for_ccf = current_user_actual_history if current_user_actual_history is not None else user_history.get(user_id, {})
    for sim_vid_str in similar_vehicle_ids_str:
        if sim_vid_str in hist_to_use_for_ccf:
            try: user_ratings_for_similar_items.append(float(hist_to_use_for_ccf[sim_vid_str]))
            except (ValueError, TypeError): pass
    if not user_ratings_for_similar_items:
        if hist_to_use_for_ccf:
            try:
                valid_ratings_in_hist = [float(r) for r in hist_to_use_for_ccf.values() if isinstance(r, (int, float, str)) and str(r).replace('.', '', 1).isdigit()]
                if valid_ratings_in_hist: return np.clip(np.mean(valid_ratings_in_hist), MIN_RATING, MAX_RATING)
            except Exception: pass
        return user_specific_mean
    predicted_rating = np.mean(user_ratings_for_similar_items)
    return np.clip(predicted_rating, MIN_RATING, MAX_RATING)

# эндпоинт объекта
@app.route('/api/tf-recs/<string:uid>', methods=['GET'])
@jwt_required(optional=True)
def tf_recs(uid):
    if not user_lookup or not user_model or item_embeddings_norm is None or not tf_item_ids:
        app.logger.error(f"TF components not available for tf-recs for user {uid}. Aborting.")
        return jsonify([])
    try:
        idx = user_lookup(tf.constant([str(uid)])).numpy()[0]
        emb = user_model.predict(np.array([idx]))[0]
        emb = emb / (np.linalg.norm(emb)+1e-10)
        scores = item_embeddings_norm.dot(emb)
        k=10
        topk = np.argpartition(-scores, k)[:k]
        topk = topk[np.argsort(-scores[topk])]
        rec_ids = [tf_item_ids[i] for i in topk]
    except Exception as e:
        app.logger.error(f"tf-recs failed for {uid}: {e}", exc_info=True)
        return jsonify([])

    # загрузка данных об автомобиле
    vehicles = Vehicle.query.filter(Vehicle.vehicle_id.in_(rec_ids)).all()
    vmap = {v.vehicle_id:v for v in vehicles}
    out = []
    for vid in rec_ids:
        v = vmap.get(vid)
        if not v: continue
        out.append({
            'vehicle_id': v.vehicle_id,
            'brand':       v.brand,
            'model':       v.model,
            'body_type':   v.body_type,
            'year':        v.year,
            'gearbox':     v.gearbox,
            'drive':       v.drive,
            'power_hp':    v.power_hp,
            'seats':       v.seats,
            'price_per_day_usd': float(v.price_per_day_usd),
            'image_url':   v.image_url
        })
    return jsonify(out)

# эндпоинт пользователя
@app.route('/api/auth/register', methods=['POST'])
def register():
    data = request.get_json(silent=True)
    if not data: return jsonify({'error': 'Request body must be JSON and not empty'}), 400
    username, password = data.get('username'), data.get('password')
    if not username or not password: return jsonify({'error': 'Username and password required'}), 400
    if User.query.filter_by(username=username).first(): return jsonify({'error': 'User already exists'}), 409
    role = Role.query.filter_by(name='user').first()
    if not role:
        app.logger.error("CRITICAL: 'user' role not found during registration.")
        return jsonify({'error': 'Server configuration error, please try again later.'}), 500
    pw_hash = bcrypt.generate_password_hash(password).decode('utf-8')
    user = User(username=username, password_hash=pw_hash, role_id=role.id)
    db.session.add(user); db.session.commit()
    app.logger.info(f"User '{username}' registered with ID {user.id} and role '{role.name}'.")
    return jsonify({'message': 'Registered successfully. Please login.'}), 201


@app.route('/api/auth/login', methods=['POST'])
def login():
    data = request.get_json(silent=True)
    if not data: return jsonify({'error': 'Request body must be JSON and not empty'}), 400
    username, password = data.get('username'), data.get('password')
    if not username or not password: return jsonify({'error': 'Username and password required'}), 400
    user = User.query.filter_by(username=username).first()
    if not user or not bcrypt.check_password_hash(user.password_hash, password):
        app.logger.warning(f"Login attempt failed for username: {username}")
        return jsonify({'error': 'Invalid username or password'}), 401
    additional_claims = {"role": user.role.name if user.role else "user"}
    if not user.role: app.logger.error(f"User {user.id} ('{user.username}') has no role assigned at login!")
    token = create_access_token(identity=str(user.id), additional_claims=additional_claims)
    app.logger.info(f"User '{username}' (ID: {user.id}, Role: {additional_claims.get('role')}) logged in.")
    return jsonify({'token': token}), 200

@app.route('/api/vehicles', methods=['GET'])
def get_vehicles():
    vehicles_query = db.session.query(
        Vehicle,
        func.avg(Review.rating).label('avg_rating'),
        func.count(Review.id).label('review_count')
    ).outerjoin(Review, Vehicle.vehicle_id == Review.vehicle_id) \
     .group_by(Vehicle.vehicle_id).all()

    result = [{
        'vehicle_id': v.vehicle_id,
        'brand': v.brand,
        'model': v.model,
        'body_type': v.body_type,
        'year': v.year,
        'gearbox': v.gearbox,
        'drive': v.drive,
        'power_hp': v.power_hp,
        'seats': v.seats,
        'price_per_day_usd': float(v.price_per_day_usd),
        'image_url': v.image_url,
        'avg_rating': float(avg_rating) if avg_rating is not None else None,
        'review_count': review_count
    } for v, avg_rating, review_count in vehicles_query]
    return jsonify(result)

@app.route('/api/vehicles/<string:vid>', methods=['GET'])
def get_vehicle(vid):
    vehicle_data = db.session.query(
        Vehicle,
        func.avg(Review.rating).label('avg_rating'),
        func.count(Review.id).label('review_count')
    ).outerjoin(Review, Vehicle.vehicle_id == Review.vehicle_id) \
     .filter(Vehicle.vehicle_id == vid) \
     .group_by(Vehicle.vehicle_id).first()

    if not vehicle_data:
        return jsonify({'error': 'Vehicle not found'}), 404

    v, avg_rating, review_count = vehicle_data
    return jsonify({
        'vehicle_id': v.vehicle_id,
        'brand': v.brand,
        'model': v.model,
        'body_type': v.body_type,
        'year': v.year,
        'gearbox': v.gearbox,
        'drive': v.drive,
        'power_hp': v.power_hp,
        'seats': v.seats,
        'price_per_day_usd': float(v.price_per_day_usd),
        'image_url': v.image_url,
        'avg_rating': float(avg_rating) if avg_rating is not None else None,
        'review_count': review_count
    })

@app.route('/api/vehicles/<string:vid>/reviews', methods=['GET'])
def get_vehicle_reviews(vid):
    if not db.session.get(Vehicle, vid): return jsonify({'error': 'Vehicle not found'}), 404
    reviews_q = Review.query.filter_by(vehicle_id=vid).order_by(Review.created_at.desc()).all()
    return jsonify([{'id': r.id, 'user_id': r.user_id, 'vehicle_id': r.vehicle_id, 'order_id': r.order_id,
                     'rating': r.rating, 'created_at': r.created_at.isoformat(), 'comment': r.comment
                    } for r in reviews_q])

@app.route('/api/cart', methods=['GET', 'POST'])
@jwt_required()
def manage_cart():
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    if request.method == 'GET':
        items = CartItem.query.filter_by(user_id=user_id).all()
        return jsonify([{'vehicle_id': i.vehicle_id, 'brand': i.vehicle.brand, 'model': i.vehicle.model,
                         'price_per_day_usd': float(i.vehicle.price_per_day_usd), 'image_url': i.vehicle.image_url
                        } for i in items])
    data = request.get_json(silent=True)
    if not data: return jsonify({'error': 'Request body must be JSON'}), 400
    vid_str = data.get('vehicleId')
    if not isinstance(vid_str, str) or not vid_str.strip(): return jsonify({'error': 'Invalid vehicleId, must be a non-empty string'}), 400
    if not db.session.get(Vehicle, vid_str): return jsonify({'error': f'Vehicle {vid_str} not found'}), 404
    if CartItem.query.filter_by(user_id=user_id, vehicle_id=vid_str).first(): return jsonify({'error': 'Item already in cart'}), 409
    db.session.add(CartItem(user_id=user_id, vehicle_id=vid_str)); db.session.commit()
    items_after_add = CartItem.query.filter_by(user_id=user_id).all()
    return jsonify([{'vehicle_id': i.vehicle_id, 'brand': i.vehicle.brand, 'model': i.vehicle.model,
                     'price_per_day_usd': float(i.vehicle.price_per_day_usd), 'image_url': i.vehicle.image_url
                    } for i in items_after_add]), 201

@app.route('/api/cart/<string:vid>', methods=['DELETE'])
@jwt_required()
def remove_cart(vid):
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    item = CartItem.query.filter_by(user_id=user_id, vehicle_id=vid).first()
    if not item: return jsonify({'error': 'Item not in cart'}), 404
    db.session.delete(item); db.session.commit()
    items_after_delete = CartItem.query.filter_by(user_id=user_id).all()
    return jsonify([{'vehicle_id': i.vehicle_id, 'brand': i.vehicle.brand, 'model': i.vehicle.model,
                     'price_per_day_usd': float(i.vehicle.price_per_day_usd), 'image_url': i.vehicle.image_url
                    } for i in items_after_delete]), 200

@app.route('/api/checkout', methods=['POST'])
@jwt_required()
def checkout():
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    items = CartItem.query.filter_by(user_id=user_id).all()
    if not items: return jsonify({'error': 'Cart empty'}), 400
    order = Order(user_id=user_id)
    db.session.add(order); db.session.flush()
    for ci in items:
        db.session.add(OrderItem(order_id=order.id, vehicle_id=ci.vehicle_id, price=ci.vehicle.price_per_day_usd))
        db.session.delete(ci)
    db.session.commit()
    app.logger.info(f"Checkout successful for user {user_id}, order {order.id}. Cart cleared.")
    return get_history()

@app.route('/api/history', methods=['GET'])
@jwt_required()
def get_history():
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    orders_q = Order.query.filter_by(user_id=user_id).order_by(Order.created_at.desc()).all()
    return jsonify([{'order_id': o.id, 'created_at': o.created_at.isoformat(),
                     'items': [{'vehicle_id': it.vehicle_id, 'brand': it.vehicle.brand,
                                'model': it.vehicle.model, 'price': float(it.price),
                                'image_url': it.vehicle.image_url } for it in o.items]
                    } for o in orders_q])

@app.route('/api/reviews', methods=['GET'])
@jwt_required()
def list_my_reviews():
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    revs_q = db.session.query(Review, Vehicle.brand, Vehicle.model, Vehicle.image_url)\
        .join(Vehicle, Review.vehicle_id == Vehicle.vehicle_id)\
        .filter(Review.user_id == user_id).order_by(Review.created_at.desc()).all()
    return jsonify([{'id': r.id, 'vehicle_id': r.vehicle_id,
                     'vehicle_brand': brand, 'vehicle_model': model, 'vehicle_image_url': image_url,
                     'order_id': r.order_id, 'rating': r.rating, 'created_at': r.created_at.isoformat(),
                     'comment': r.comment} for r, brand, model, image_url in revs_q])

@app.route('/api/reviews', methods=['POST'])
@jwt_required()
def add_review():
    try:
        user_id = get_current_user_id()
    except ValueError as e:
        app.logger.error(f"AddReview: Invalid user ID in token: {e}")
        return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400

    data = request.get_json(silent=True)
    app.logger.debug(f"AddReview: Received data for user {user_id}: {data}")

    if not data:
        app.logger.warning(f"AddReview: User {user_id} - Request body is not JSON or empty.")
        return jsonify({'error': 'Request body must be JSON and not empty'}), 400

    vid_str = data.get('vehicle_id')
    oid = data.get('order_id')
    rating_val = data.get('rating')
    comment_text = data.get('comment')

    if not isinstance(vid_str, str) or not vid_str.strip():
        app.logger.warning(f"AddReview: User {user_id} - Missing or invalid vehicle_id: {vid_str}")
        return jsonify({'error': 'Missing or invalid type for field: vehicle_id (must be string)'}), 400
    if not isinstance(oid, int):
        app.logger.warning(f"AddReview: User {user_id} - Missing or invalid order_id type: {oid} (type: {type(oid)})")
        return jsonify({'error': 'Missing or invalid type for field: order_id (must be integer)'}), 400
    if not isinstance(rating_val, (int, float)):
        app.logger.warning(f"AddReview: User {user_id} - Missing or invalid rating type: {rating_val} (type: {type(rating_val)})")
        return jsonify({'error': 'Missing or invalid type for field: rating (must be number)'}), 400
    if not (MIN_RATING <= rating_val <= MAX_RATING):
        app.logger.warning(f"AddReview: User {user_id} - Rating out of range: {rating_val}")
        return jsonify({'error': f'Rating must be between {MIN_RATING} and {MAX_RATING}'}), 400

    order = db.session.get(Order, oid)
    if not order:
        app.logger.warning(f"AddReview: User {user_id} - Order ID {oid} not found.")
        return jsonify({'error': 'Order not found'}), 404
    if order.user_id != user_id:
        app.logger.warning(f"AddReview: User {user_id} - Order ID {oid} does not belong to this user (belongs to {order.user_id}).")
        return jsonify({'error': 'Invalid order or not user order'}), 403

    if not any(it.vehicle_id == vid_str for it in order.items):
        app.logger.warning(f"AddReview: User {user_id} - Vehicle ID {vid_str} not found in order ID {oid}.")
        return jsonify({'error': 'Vehicle not in specified order'}), 400

    if Review.query.filter_by(user_id=user_id, vehicle_id=vid_str, order_id=oid).first():
        app.logger.warning(f"AddReview: User {user_id} - Review for vehicle ID {vid_str} in order ID {oid} already exists.")
        return jsonify({'error': 'Review for this item in this order already exists'}), 409

    r = Review(user_id=user_id, vehicle_id=vid_str, order_id=oid, rating=rating_val, comment=comment_text)
    db.session.add(r)
    db.session.commit()
    app.logger.info(f"AddReview: User {user_id} - Review ID {r.id} created successfully for vehicle {vid_str}, order {oid}.")

    v_info = db.session.get(Vehicle, vid_str)
    return jsonify({
        'id': r.id, 'vehicle_id': r.vehicle_id, 'order_id': r.order_id, 'rating': r.rating,
        'created_at': r.created_at.isoformat(), 'comment': r.comment,
        'vehicle_brand': v_info.brand if v_info else None,
        'vehicle_model': v_info.model if v_info else None,
        'vehicle_image_url': v_info.image_url if v_info else None
    }), 201


@app.route('/api/reviews/<int:rid>', methods=['PUT'])
@jwt_required()
def update_review(rid):
    try:
        user_id = get_current_user_id()
    except ValueError as e:
        app.logger.error(f"UpdateReview: Invalid user ID in token: {e}")
        return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400

    data = request.get_json(silent=True)
    app.logger.debug(f"UpdateReview: Received data for review ID {rid}, user {user_id}: {data}")

    if not data:
        app.logger.warning(f"UpdateReview: User {user_id}, Review ID {rid} - Request body is not JSON or empty.")
        return jsonify({'error': 'Request body must be JSON and not empty'}), 400

    rev = db.session.get(Review, rid)
    if not rev:
        app.logger.warning(f"UpdateReview: User {user_id} - Review ID {rid} not found.")
        return jsonify({'error': 'Review not found'}), 404
    if rev.user_id != user_id:
        app.logger.warning(f"UpdateReview: User {user_id} - Review ID {rid} does not belong to this user (belongs to {rev.user_id}).")
        return jsonify({'error': 'Review not found or not owned by user'}), 403

    if 'rating' not in data:
        app.logger.warning(f"UpdateReview: User {user_id}, Review ID {rid} - Missing 'rating' field in request.")
        return jsonify({'error': 'Missing rating field for update'}), 400

    try:
        rating_val = data['rating']
        if not isinstance(rating_val, (int, float)):
            app.logger.warning(f"UpdateReview: User {user_id}, Review ID {rid} - Invalid rating type: {rating_val}")
            raise ValueError("Rating must be a number.")
        if not (MIN_RATING <= rating_val <= MAX_RATING):
            app.logger.warning(f"UpdateReview: User {user_id}, Review ID {rid} - Rating out of range: {rating_val}")
            raise ValueError(f"Rating must be between {MIN_RATING} and {MAX_RATING}.")
    except (ValueError, TypeError) as e:
        return jsonify({'error': f'Invalid rating: {e}'}), 400

    rev.rating = rating_val
    if 'comment' in data:
        rev.comment = data.get('comment', rev.comment)

    db.session.commit()
    app.logger.info(f"UpdateReview: User {user_id} - Review ID {rid} updated successfully.")

    v_info = db.session.get(Vehicle, rev.vehicle_id)
    return jsonify({
        'id': rev.id, 'vehicle_id': rev.vehicle_id, 'order_id': rev.order_id, 'rating': rev.rating,
        'created_at': rev.created_at.isoformat(), 'comment': rev.comment,
        'vehicle_brand': v_info.brand if v_info else None,
        'vehicle_model': v_info.model if v_info else None,
        'vehicle_image_url': v_info.image_url if v_info else None
    }), 200

@app.route('/api/reviews/<int:rid>', methods=['DELETE'])
@jwt_required()
def delete_review(rid):
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    r = db.session.get(Review, rid)
    if not r or r.user_id != user_id: return jsonify({'error': 'Review not found or not owned'}), 404
    db.session.delete(r); db.session.commit()
    app.logger.info(f"DeleteReview: User {user_id} - Review ID {rid} deleted.")
    return '', 204


@app.route('/api/recommendations', methods=['GET'])
@jwt_required(optional=True)
def get_general_recommendations():
    user_id = None
    try:
        if get_jwt_identity(): user_id = get_current_user_id()
    except Exception: user_id = None
    recent_vids_str = set()
    MIN_ITEMS_FOR_SIMILARITY = 1
    MAX_SOURCE_ITEMS = 3
    if user_id:
        orders = Order.query.filter_by(user_id=user_id).order_by(Order.created_at.desc()).limit(MAX_SOURCE_ITEMS).all()
        for o in orders:
            for item in o.items: recent_vids_str.add(str(item.vehicle_id))
            if len(recent_vids_str) >= MAX_SOURCE_ITEMS: break
        if len(recent_vids_str) < MIN_ITEMS_FOR_SIMILARITY:
            cart_items = CartItem.query.filter_by(user_id=user_id).limit(MAX_SOURCE_ITEMS - len(recent_vids_str)).all()
            for item in cart_items: recent_vids_str.add(str(item.vehicle_id))
    if not recent_vids_str or len(recent_vids_str) < MIN_ITEMS_FOR_SIMILARITY :
        app.logger.info(f"GeneralRecommendations: User {user_id if user_id else 'Guest'} has insufficient history. Serving popular items.")
        pop_recs_q = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
            .outerjoin(Review).group_by(Vehicle.vehicle_id)\
            .order_by(func.count(Review.id).desc(), func.avg(Review.rating).desc().nullslast()).limit(8).all()
        return jsonify([{'vehicle_id': v.vehicle_id, 'brand': v.brand, 'model': v.model, 'body_type': v.body_type,
                         'year': v.year, 'gearbox': v.gearbox, 'drive': v.drive, 'power_hp': v.power_hp, 'seats': v.seats,
                         'price_per_day_usd': float(v.price_per_day_usd), 'image_url': v.image_url,
                         'avg_rating': float(avg_r) if avg_r else None, 'review_count': rev_c} for v, avg_r, rev_c in pop_recs_q])
    recent_vehicles_objs = Vehicle.query.filter(Vehicle.vehicle_id.in_(list(recent_vids_str))).all()
    body_types = list(set(v.body_type for v in recent_vehicles_objs if v.body_type))
    brands = list(set(v.brand for v in recent_vehicles_objs if v.brand))
    seen_vids_for_this_user_query_str = recent_vids_str.copy()
    if user_id:
        seen_vids_for_this_user_query_str.update(item.vehicle_id for item in CartItem.query.filter_by(user_id=user_id).all())
        all_ordered_items = OrderItem.query.join(Order).filter(Order.user_id == user_id).all()
        seen_vids_for_this_user_query_str.update(str(item.vehicle_id) for item in all_ordered_items)
    q_filters = []
    if body_types: q_filters.append(Vehicle.body_type.in_(body_types))
    if brands: q_filters.append(Vehicle.brand.in_(brands))
    if not q_filters:
        app.logger.warning(f"GeneralRecommendations: User {user_id} - no criteria from history. Falling back to popular.")
        pop_recs_q = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
            .outerjoin(Review).group_by(Vehicle.vehicle_id)\
            .order_by(func.count(Review.id).desc(), func.avg(Review.rating).desc().nullslast()).limit(8).all()
        return jsonify([{'vehicle_id': v.vehicle_id, 'brand': v.brand, 'model': v.model, 'body_type': v.body_type,
                         'year': v.year, 'gearbox': v.gearbox, 'drive': v.drive, 'power_hp': v.power_hp, 'seats': v.seats,
                         'price_per_day_usd': float(v.price_per_day_usd), 'image_url': v.image_url,
                         'avg_rating': float(avg_r) if avg_r else None, 'review_count': rev_c} for v, avg_r, rev_c in pop_recs_q])
    recs_q_builder = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
        .outerjoin(Review).filter(or_(*q_filters))
    if seen_vids_for_this_user_query_str:
        recs_q_builder = recs_q_builder.filter(~Vehicle.vehicle_id.in_(list(seen_vids_for_this_user_query_str)))
    final_recs_q = recs_q_builder.group_by(Vehicle.vehicle_id)\
        .order_by(func.count(Review.id).desc(), func.avg(Review.rating).desc().nullslast(), func.random()).limit(4).all()
    if not final_recs_q:
        app.logger.info(f"GeneralRecommendations: User {user_id} - no recommendations from history similarity. Falling back to popular.")
        pop_recs_q = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
            .outerjoin(Review).group_by(Vehicle.vehicle_id)\
            .order_by(func.count(Review.id).desc(), func.avg(Review.rating).desc().nullslast()).limit(4).all()
        final_recs_q = pop_recs_q
    return jsonify([{'vehicle_id': v.vehicle_id, 'brand': v.brand, 'model': v.model, 'body_type': v.body_type,
                     'year': v.year, 'gearbox': v.gearbox, 'drive': v.drive, 'power_hp': v.power_hp, 'seats': v.seats,
                     'price_per_day_usd': float(v.price_per_day_usd), 'image_url': v.image_url,
                     'avg_rating': float(avg_r) if avg_r else None, 'review_count': rev_c} for v, avg_r, rev_c in final_recs_q])

@app.route('/api/recommendations/personal', methods=['GET'])
@jwt_required()
def recommendations_personal_revised():
    try: user_id = get_current_user_id()
    except ValueError as e: return jsonify(msg=str(e), error_code="INVALID_USER_ID"), 400
    app.logger.debug(f"PersonalRecommendations: Request for user_id: {user_id}")
    user_actual_reviews_db = Review.query.filter_by(user_id=user_id).all()
    current_user_actual_history_for_prediction = {str(r.vehicle_id): float(r.rating) for r in user_actual_reviews_db}
    MIN_REVIEWS_FOR_PERSONAL = 1
    if not current_user_actual_history_for_prediction or len(current_user_actual_history_for_prediction) < MIN_REVIEWS_FOR_PERSONAL:
        app.logger.info(f"PersonalRecommendations: User {user_id} has {len(current_user_actual_history_for_prediction)} actual reviews. Returning empty list.")
        return jsonify([])
    app.logger.debug(f"PersonalRecommendations: User {user_id} has {len(current_user_actual_history_for_prediction)} items in actual history.")

    hist_vids = list(current_user_actual_history_for_prediction.keys())
    hist_vehicles = db.session.query(Vehicle).filter(Vehicle.vehicle_id.in_(hist_vids)).all()

    if not hist_vehicles:
        app.logger.warning(f"PersonalRecommendations: User {user_id} - No vehicle objects found for historical VIDs. Returning empty list.")
        return jsonify([])

    brands = {v.brand for v in hist_vehicles if v.brand}
    body_types = {v.body_type for v in hist_vehicles if v.body_type}
    avg_year = sum(v.year for v in hist_vehicles) / len(hist_vehicles) if hist_vehicles else datetime.now().year
    avg_power = sum(v.power_hp for v in hist_vehicles) / len(hist_vehicles) if hist_vehicles else 0

    year_min = int(avg_year) - 6
    year_max = int(avg_year) + 6
    power_min = int(avg_power) - 80
    power_max = int(avg_power) + 80

    all_db_vids_str = [str(v.vehicle_id) for v in db.session.query(Vehicle.vehicle_id).all()]
    seen_vids_user_str = set(hist_vids)
    for order in Order.query.filter_by(user_id=user_id).all():
        for item in order.items: seen_vids_user_str.add(str(item.vehicle_id))
    for item in CartItem.query.filter_by(user_id=user_id).all(): seen_vids_user_str.add(str(item.vehicle_id))

    candidate_vids_str_query = db.session.query(Vehicle.vehicle_id)\
        .filter(~Vehicle.vehicle_id.in_(list(seen_vids_user_str)))

    if brands: candidate_vids_str_query = candidate_vids_str_query.filter(Vehicle.brand.in_(brands))
    if body_types: candidate_vids_str_query = candidate_vids_str_query.filter(Vehicle.body_type.in_(body_types))
    candidate_vids_str_query = candidate_vids_str_query.filter(Vehicle.year.between(year_min, year_max))
    candidate_vids_str_query = candidate_vids_str_query.filter(Vehicle.power_hp.between(power_min, power_max))

    candidate_vids_str = [vid for (vid,) in candidate_vids_str_query.all()]

    app.logger.debug(f"PersonalRecommendations: User {user_id} - candidates after attribute filter: {len(candidate_vids_str)}")

    if not candidate_vids_str:
        app.logger.info(f"PersonalRecommendations: User {user_id} - no candidate vehicles left after filtering.")
        return jsonify([])

    preds = []
    for vid_str in candidate_vids_str:
        ucf_p = predict_user_cf_revised(user_id, vid_str, current_user_actual_history_for_prediction)
        ccf_p = predict_content_cf_revised(user_id, vid_str, current_user_actual_history_for_prediction)
        ucf_p = global_mean if pd.isna(ucf_p) else ucf_p
        ccf_p = global_mean if pd.isna(ccf_p) else ccf_p
        hybrid_p = best_alpha * ucf_p + (1.0 - best_alpha) * ccf_p
        preds.append({'vehicle_id': vid_str, 'score': hybrid_p})

    FINAL_N_RECS = 10
    sorted_preds_by_score = sorted(preds, key=lambda x: x['score'], reverse=True)
    top_cand_ids_str = [p['vehicle_id'] for p in sorted_preds_by_score[:FINAL_N_RECS * 2]]

    if not top_cand_ids_str:
        app.logger.info(f"PersonalRecommendations: User {user_id} - no candidates after prediction scoring.")
        return jsonify([])

    recs_q = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
        .outerjoin(Review).filter(Vehicle.vehicle_id.in_(top_cand_ids_str)).group_by(Vehicle.vehicle_id).all()

    recs_map = {str(v.vehicle_id): {
        'data': v, 'avg_r': float(avg_r) if avg_r else None, 'rev_c': rev_c,
        'pred_score': next((p['score'] for p in sorted_preds_by_score if p['vehicle_id'] == str(v.vehicle_id)), global_mean)
        } for v, avg_r, rev_c in recs_q}

    enriched_recs_list = []
    for p_item in sorted_preds_by_score[:FINAL_N_RECS*2]:
        p_item_id_str = p_item['vehicle_id']
        if p_item_id_str in recs_map:
            entry = recs_map[p_item_id_str]
            v_obj, avg_r_val, rev_c_val = entry['data'], entry['avg_r'], entry['rev_c']
            pred_score_val = entry['pred_score']
            enriched_recs_list.append({
                'vehicle_id': v_obj.vehicle_id, 'brand': v_obj.brand, 'model': v_obj.model,
                'body_type': v_obj.body_type, 'year': v_obj.year, 'gearbox': v_obj.gearbox,
                'drive': v_obj.drive, 'power_hp': v_obj.power_hp, 'seats': v_obj.seats,
                'price_per_day_usd': float(v_obj.price_per_day_usd), 'image_url': v_obj.image_url,
                'predicted_score_for_user': round(pred_score_val, 2),
                'avg_rating': avg_r_val, 'review_count': rev_c_val })
            if len(enriched_recs_list) >= FINAL_N_RECS:
                break

    app.logger.debug(f"PersonalRecommendations: User {user_id} - final recs count: {len(enriched_recs_list)}")
    return jsonify(enriched_recs_list)

@app.route('/api/vehicles/<string:vehicle_id_target>/recommendations', methods=['GET'])
def recommendations_by_vehicle_revised(vehicle_id_target):
    app.logger.debug(f"SimilarVehicles: Request for target_id: {vehicle_id_target}")
    if item_knn is None or item_pipeline is None or X_items_index is None or not X_ITEMS_PIPELINE_COLS:
        app.logger.warning("SimilarVehicles: Core artifacts missing for content-based similar items. Returning [].")
        return jsonify([])

    target_v_db = db.session.get(Vehicle, vehicle_id_target)
    if not target_v_db:
        app.logger.warning(f"SimilarVehicles: Target vehicle ID {vehicle_id_target} not in DB. Returning [].")
        return jsonify([])
    app.logger.debug(f"SimilarVehicles: Target vehicle: {target_v_db.brand} {target_v_db.model}")

    year_min_attr = target_v_db.year - 6
    year_max_attr = target_v_db.year + 6
    power_min_attr = target_v_db.power_hp - 80
    power_max_attr = target_v_db.power_hp + 80

    attribute_candidate_vids_query = db.session.query(Vehicle.vehicle_id) \
        .filter(Vehicle.vehicle_id != vehicle_id_target)

    if target_v_db.brand:
        attribute_candidate_vids_query = attribute_candidate_vids_query.filter(Vehicle.brand == target_v_db.brand)
    if target_v_db.body_type:
        attribute_candidate_vids_query = attribute_candidate_vids_query.filter(Vehicle.body_type == target_v_db.body_type)

    attribute_candidate_vids_query = attribute_candidate_vids_query.filter(Vehicle.year.between(year_min_attr, year_max_attr))
    attribute_candidate_vids_query = attribute_candidate_vids_query.filter(Vehicle.power_hp.between(power_min_attr, power_max_attr))

    attribute_candidate_vids = {vid for (vid,) in attribute_candidate_vids_query.limit(50).all()}
    app.logger.debug(f"SimilarVehicles: {len(attribute_candidate_vids)} candidates after attribute filtering for {vehicle_id_target}.")


    item_feats_dict = {}
    for feat_name in X_ITEMS_PIPELINE_COLS:
        if feat_name == 'model_grouped':
            raw_model = getattr(target_v_db, 'model', None)
            item_feats_dict['model_grouped'] = raw_model if raw_model and raw_model in TOP_MODELS_FOR_GROUPING else 'Other_Model'
        else: item_feats_dict[feat_name] = getattr(target_v_db, feat_name, None)

    app.logger.debug(f"SimilarVehicles: Features for pipeline for target {vehicle_id_target}: {item_feats_dict}")
    try:
        item_feats_df = pd.DataFrame([item_feats_dict])[X_ITEMS_PIPELINE_COLS]
        transformed_v_vec = item_pipeline.transform(item_feats_df)
        app.logger.debug(f"SimilarVehicles: Target item vector transformed. Shape: {transformed_v_vec.shape}")
    except Exception as e:
        app.logger.error(f"SimilarVehicles: Error in item_pipeline.transform for target {vehicle_id_target}: {e}", exc_info=True)
        return jsonify([])

    num_items_in_X_items_index = len(X_items_index) if X_items_index is not None else 0
    FINAL_N_RECS_CB = 4
    desired_neighbors_for_query = FINAL_N_RECS_CB * 5 + 1

    k_param_query = min(desired_neighbors_for_query,
                        item_knn.n_neighbors if hasattr(item_knn, 'n_neighbors') else desired_neighbors_for_query,
                        num_items_in_X_items_index)

    if k_param_query <= 0 :
        app.logger.warning(f"SimilarVehicles: k_param_query for KNN is {k_param_query}. Insufficient items in KNN index. Returning [].")
        return jsonify([])

    app.logger.debug(f"SimilarVehicles: Querying KNN with k_param_query = {k_param_query}")
    try:
        distances, neigh_indices_flat = item_knn.kneighbors(transformed_v_vec, n_neighbors=k_param_query)
        neigh_indices_flat = neigh_indices_flat.flatten()
        app.logger.debug(f"SimilarVehicles: KNN Neighbors found. Indices: {neigh_indices_flat}, Distances: {distances.flatten()}")
    except Exception as e:
        app.logger.error(f"SimilarVehicles: Error in item_knn.kneighbors for target {vehicle_id_target}: {e}", exc_info=True)
        return jsonify([])

    knn_similar_vids_list_str = []
    for i, idx_in_X_items_index in enumerate(neigh_indices_flat):
        if 0 <= idx_in_X_items_index < num_items_in_X_items_index:
            item_id_from_artifact = X_items_index[idx_in_X_items_index]
            if item_id_from_artifact != vehicle_id_target:
                if attribute_candidate_vids and item_id_from_artifact in attribute_candidate_vids:
                    knn_similar_vids_list_str.append(item_id_from_artifact)
                elif not attribute_candidate_vids:
                    knn_similar_vids_list_str.append(item_id_from_artifact)

        if len(knn_similar_vids_list_str) >= FINAL_N_RECS_CB:
            break
    
    if len(knn_similar_vids_list_str) < FINAL_N_RECS_CB:
        app.logger.debug(f"SimilarVehicles: Filling remaining slots from KNN results as attribute-filtered KNN count is {len(knn_similar_vids_list_str)}")
        for i, idx_in_X_items_index in enumerate(neigh_indices_flat):
            if 0 <= idx_in_X_items_index < num_items_in_X_items_index:
                item_id_from_artifact = X_items_index[idx_in_X_items_index]
                if item_id_from_artifact != vehicle_id_target and item_id_from_artifact not in knn_similar_vids_list_str:
                    knn_similar_vids_list_str.append(item_id_from_artifact)
            if len(knn_similar_vids_list_str) >= FINAL_N_RECS_CB:
                break


    app.logger.debug(f"SimilarVehicles: Final similar vehicle IDs (max {FINAL_N_RECS_CB}): {knn_similar_vids_list_str}")
    top_sim_ids_str = knn_similar_vids_list_str[:FINAL_N_RECS_CB]

    if not top_sim_ids_str:
        app.logger.info(f"SimilarVehicles: No similar vehicles found for {vehicle_id_target} after KNN and filtering. Returning [].")
        return jsonify([])

    recs_db = db.session.query(Vehicle, func.avg(Review.rating).label('avg_r'), func.count(Review.id).label('rev_c'))\
        .outerjoin(Review).filter(Vehicle.vehicle_id.in_(top_sim_ids_str)).group_by(Vehicle.vehicle_id).all()

    recs_map_cb = {str(v.vehicle_id): {'data': v, 'avg_r': avg_r, 'rev_c': rev_c} for v, avg_r, rev_c in recs_db}

    final_recs_cb_list = []
    for vid_str_sim in top_sim_ids_str:
        if vid_str_sim in recs_map_cb:
            entry = recs_map_cb[vid_str_sim]
            v_obj, avg_r, rev_c = entry['data'], entry['avg_r'], entry['rev_c']
            final_recs_cb_list.append({
                'vehicle_id': v_obj.vehicle_id, 'brand': v_obj.brand, 'model': v_obj.model, 'body_type': v_obj.body_type,
                'year': v_obj.year, 'gearbox': v_obj.gearbox, 'drive': v_obj.drive, 'power_hp': v_obj.power_hp, 'seats': v_obj.seats,
                'price_per_day_usd': float(v_obj.price_per_day_usd), 'image_url': v_obj.image_url,
                'avg_rating': float(avg_r) if avg_r else None, 'review_count': rev_c})
    app.logger.debug(f"SimilarVehicles: Final recommendations count: {len(final_recs_cb_list)} for target {vehicle_id_target}")
    return jsonify(final_recs_cb_list)

@app.route('/api/admin/users', methods=['GET'])
@admin_required
def admin_get_users():
    try:
        users_query = User.query.join(Role).options(db.joinedload(User.role)).order_by(User.id).all()
        return jsonify([{'id': u.id, 'username': u.username, 'role': u.role.name if u.role else 'N/A'
                        } for u in users_query])
    except Exception as e:
        app.logger.error(f"Error in admin_get_users: {e}", exc_info=True)
        return jsonify(error="Failed to retrieve users"), 500

@app.route('/api/admin/users/<int:user_id_to_update>', methods=['PUT'])
@admin_required
def admin_update_user(user_id_to_update):
    user = User.query.get_or_404(user_id_to_update)
    data = request.get_json()
    if not data: return jsonify({'error': 'Request body must be JSON'}), 400
    try:
        if 'username' in data:
            new_username = data['username'].strip()
            if not new_username: return jsonify({'error': 'Username cannot be empty'}), 400
            if new_username != user.username and User.query.filter(User.id != user_id_to_update, User.username == new_username).first():
                return jsonify({'error': 'Username already taken'}), 409
            user.username = new_username
        if 'password' in data and data['password']:
            if len(data['password']) < 4: return jsonify({'error': 'Password must be at least 4 characters long'}), 400
            user.password_hash = bcrypt.generate_password_hash(data['password']).decode('utf-8')
            app.logger.info(f"Admin updated password for user ID: {user_id_to_update}")
        if 'role' in data:
            new_role_name = data['role']
            current_admin_id = get_current_user_id()
            if user.id == current_admin_id and user.role.name == 'admin' and new_role_name != 'admin':
                admin_count = User.query.join(Role).filter(Role.name == 'admin').count()
                if admin_count <= 1: return jsonify({'error': 'Cannot remove admin role from the last administrator.'}), 403
            role_obj = Role.query.filter_by(name=new_role_name).first()
            if not role_obj: return jsonify({'error': f'Role "{new_role_name}" not found'}), 400
            user.role_id = role_obj.id
        db.session.commit()
        app.logger.info(f"Admin updated user ID: {user_id_to_update}. Username: {user.username}, Role: {user.role.name}")
        return jsonify({'id': user.id, 'username': user.username, 'role': user.role.name if user.role else 'N/A'})
    except ValueError as e:
        app.logger.error(f"Admin update user: Error parsing current admin ID from JWT: {e}")
        return jsonify({'error': 'Internal server error processing admin identity.'}), 500
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error updating user {user_id_to_update}: {e}", exc_info=True)
        return jsonify(error="Failed to update user details."), 500

@app.route('/api/admin/users/<int:user_id_to_delete>', methods=['DELETE'])
@admin_required
def admin_delete_user(user_id_to_delete):
    try:
        current_admin_id = get_current_user_id()
        if user_id_to_delete == current_admin_id:
            return jsonify({'error': 'You cannot delete your own account.'}), 403

        user_to_del = User.query.get_or_404(user_id_to_delete)

        Review.query.filter_by(user_id=user_id_to_delete).delete(synchronize_session=False)
        orders = Order.query.filter_by(user_id=user_id_to_delete).all()
        for order in orders:
            OrderItem.query.filter_by(order_id=order.id).delete(synchronize_session=False)
            db.session.delete(order)

        CartItem.query.filter_by(user_id=user_id_to_delete).delete(synchronize_session=False)

        db.session.delete(user_to_del)
        db.session.commit()

        app.logger.info(f"Admin deleted user ID: {user_id_to_delete}")
        return jsonify({'message': f'User ID {user_id_to_delete} deleted successfully.'}), 200
    except ValueError as e:
        app.logger.error(f"Admin delete user: Error parsing current admin ID from JWT: {e}")
        return jsonify({'error': 'Internal server error processing admin identity.'}), 500
    except HTTPException as he:
        return jsonify({'error': he.description}), he.code
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error deleting user {user_id_to_delete}: {e}", exc_info=True)
        return jsonify({'error': 'Failed to delete user due to server error.'}), 500

@app.route('/api/admin/vehicles', methods=['GET'])
@admin_required
def admin_get_all_vehicles():
    try:
        vehicles_query = Vehicle.query.order_by(Vehicle.brand, Vehicle.model, Vehicle.year).all()
        return jsonify([{'vehicle_id': v.vehicle_id, 'brand': v.brand, 'model': v.model,
                         'body_type': v.body_type, 'year': v.year, 'gearbox': v.gearbox,
                         'drive': v.drive, 'power_hp': v.power_hp, 'seats': v.seats,
                         'price_per_day_usd': float(v.price_per_day_usd), 'image_url': v.image_url
                        } for v in vehicles_query])
    except Exception as e:
        app.logger.error(f"Error in admin_get_all_vehicles: {e}", exc_info=True)
        return jsonify(error="Failed to retrieve vehicles for admin panel."), 500

@app.route('/api/admin/vehicles', methods=['POST'])
@admin_required
@cross_origin(origins="http://localhost:5173")
def admin_add_new_vehicle():
    try:
        data = request.get_json(force=True)
    except Exception:
        return jsonify(error="Request body must be valid JSON."), 400

    if not data:
        return jsonify(error="Request body cannot be empty."), 400

    vid = data.get('vehicle_id')
    if not vid or not isinstance(vid, str) or not vid.strip():
        return jsonify(error="Field 'vehicle_id' (UUID string) is required."), 400
    vid = vid.strip()
    if Vehicle.query.get(vid):
        return jsonify(error=f"Vehicle with ID '{vid}' already exists."), 409

    required = ['brand', 'body_type', 'year', 'gearbox', 'drive', 'power_hp', 'seats', 'price_per_day_usd']
    missing = [f for f in required if data.get(f) is None or str(data.get(f)).strip() == '']
    if missing:
        return jsonify(error=f"Missing or empty required fields: {', '.join(missing)}."), 400

    try:
        brand       = str(data['brand']).strip()
        body_type   = str(data['body_type']).strip()
        year        = int(data['year'])
        gearbox     = str(data['gearbox']).strip()
        drive       = str(data['drive']).strip()
        power_hp    = int(data['power_hp'])
        seats       = int(data['seats'])
        price       = Decimal(str(data['price_per_day_usd']))
        image_url   = str(data.get('image_url','')).strip() or None
        model_val   = str(data.get('model','')).strip() or None

        if not brand:     return jsonify(error="Brand cannot be empty."), 400
        if not body_type: return jsonify(error="Body type cannot be empty."), 400
        now_year = datetime.now().year
        if not (1900 <= year <= now_year + 2):
            return jsonify(error=f"Year must be between 1900 and {now_year+2}."), 400
        if not gearbox:   return jsonify(error="Gearbox cannot be empty."), 400
        if not drive:     return jsonify(error="Drive cannot be empty."), 400
        if not (0 < power_hp < 5000):
            return jsonify(error="Power_hp must be >0 and <5000."), 400
        if not (1 <= seats <= 20):
            return jsonify(error="Seats must be between 1 and 20."), 400
        if price <= 0:
            return jsonify(error="Price_per_day_usd must be positive."), 400

        new_v = Vehicle(
            vehicle_id=vid,
            brand=brand,
            model=model_val,
            body_type=body_type,
            year=year,
            gearbox=gearbox,
            drive=drive,
            power_hp=power_hp,
            seats=seats,
            price_per_day_usd=price,
            image_url=image_url
        )
        db.session.add(new_v)
        db.session.commit()

        app.logger.info(f"Admin created vehicle {vid} ({brand} {new_v.model or ''}).")
        return jsonify({
            'vehicle_id': vid,
            'brand': brand,
            'model': new_v.model,
            'body_type': body_type,
            'year': year,
            'gearbox': gearbox,
            'drive': drive,
            'power_hp': power_hp,
            'seats': seats,
            'price_per_day_usd': float(price),
            'image_url': image_url
        }), 201

    except ValueError as ve:
        db.session.rollback()
        app.logger.warning(f"ValueError in admin_add_new_vehicle: {ve}. Payload: {data}")
        return jsonify(error=f"Invalid data type for a field: {ve}"), 400
    except HTTPException as he:
        return jsonify(error=str(he)), he.code 
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Unexpected error in admin_add_new_vehicle: {e}", exc_info=True)
        return jsonify(error="Server error while creating vehicle."), 500

@app.route('/api/admin/vehicles/<string:vehicle_id>', methods=['PUT'])
@admin_required
def admin_edit_vehicle(vehicle_id):
    vehicle = Vehicle.query.get_or_404(vehicle_id)
    data = request.get_json(silent=True)
    if not data: return jsonify({'error': 'Request body must be JSON and not empty.'}), 400
    try:
        if 'brand' in data:
            brand_val = str(data['brand']).strip()
            if not brand_val: return jsonify({'error': 'Brand cannot be empty.'}), 400
            vehicle.brand = brand_val
        if 'model' in data: vehicle.model = str(data.get('model', '')).strip() or None
        if 'body_type' in data:
            body_type_val = str(data['body_type']).strip()
            if not body_type_val: return jsonify({'error': 'Body type cannot be empty.'}), 400
            vehicle.body_type = body_type_val
        if 'year' in data:
            year_val = int(data['year'])
            now_year = datetime.now().year
            if not (1900 <= year_val <= now_year + 2): return jsonify({'error': f'Invalid year: {year_val}. Must be 1900-{now_year+2}.'}), 400
            vehicle.year = year_val
        if 'gearbox' in data:
            gearbox_val = str(data['gearbox']).strip()
            if not gearbox_val: return jsonify({'error': 'Gearbox cannot be empty.'}), 400
            vehicle.gearbox = gearbox_val
        if 'drive' in data:
            drive_val = str(data['drive']).strip()
            if not drive_val: return jsonify({'error': 'Drive type cannot be empty.'}), 400
            vehicle.drive = drive_val
        if 'power_hp' in data:
            power_hp_val = int(data['power_hp'])
            if not (0 < power_hp_val < 5000): return jsonify({'error': f'Invalid power_hp: {power_hp_val}. Must be 1-4999.'}), 400
            vehicle.power_hp = power_hp_val
        if 'seats' in data:
            seats_val = int(data['seats'])
            if not (1 <= seats_val <= 20): return jsonify({'error': f'Invalid seats: {seats_val}. Must be 1-20.'}), 400
            vehicle.seats = seats_val
        if 'price_per_day_usd' in data:
            price_val = Decimal(str(data['price_per_day_usd']))
            if not (price_val > 0): return jsonify({'error': 'Price must be a positive value.'}), 400
            vehicle.price_per_day_usd = price_val
        if 'image_url' in data: vehicle.image_url = str(data.get('image_url', '')).strip() or None

        db.session.commit()
        app.logger.info(f"Admin updated vehicle ID: {vehicle_id}.")
        return jsonify({
            'vehicle_id': vehicle.vehicle_id, 'brand': vehicle.brand, 'model': vehicle.model,
            'body_type': vehicle.body_type, 'year': vehicle.year, 'gearbox': vehicle.gearbox,
            'drive': vehicle.drive, 'power_hp': vehicle.power_hp, 'seats': vehicle.seats,
            'price_per_day_usd': float(vehicle.price_per_day_usd), 'image_url': vehicle.image_url
        })
    except ValueError as ve:
        db.session.rollback()
        app.logger.warning(f"Admin update vehicle ID {vehicle_id}: Value error - {ve}. Data: {data}")
        return jsonify({'error': f'Invalid data type for a field: {ve}.'}), 400
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error updating vehicle ID {vehicle_id}: {e}. Data: {data}", exc_info=True)
        return jsonify({'error': f'Could not update vehicle due to a server error.'}), 500

@app.route('/api/admin/vehicles/<string:vehicle_id>', methods=['DELETE'])
@admin_required
@cross_origin(origins="http://localhost:5173")
def admin_remove_vehicle(vehicle_id):
    try:
        vehicle = Vehicle.query.get_or_404(vehicle_id)

        # Порядок удаления связанных объектов
        Review.query.filter_by(vehicle_id=vehicle_id).delete(synchronize_session=False)
        CartItem.query.filter_by(vehicle_id=vehicle_id).delete(synchronize_session=False)
        OrderItem.query.filter_by(vehicle_id=vehicle_id).delete(synchronize_session=False)

        orphan_orders_cleaned_count = 0
        orders_with_this_vehicle_item = Order.query.join(OrderItem).filter(OrderItem.vehicle_id == vehicle_id).distinct(Order.id).all()
        for o in orders_with_this_vehicle_item:
            if not o.items:
                db.session.delete(o)
                orphan_orders_cleaned_count +=1
        
        db.session.delete(vehicle)
        db.session.commit()

        app.logger.info(f"Admin deleted vehicle {vehicle_id}. {orphan_orders_cleaned_count} orphan orders (if any) also cleaned.")
        return jsonify({'message': f'Vehicle {vehicle_id} deleted successfully.'}), 200

    except HTTPException as he:
        db.session.rollback()
        return jsonify({'error': he.description}), he.code
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error deleting vehicle {vehicle_id}: {e}", exc_info=True)
        return jsonify({'error': 'Failed to delete vehicle due to server error.'}), 500


if __name__ == '__main__':
    app.logger.info("Starting Flask server on 0.0.0.0:5000")
    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)
```

### `backend\create_admin.py`

```python
from app import app, db, bcrypt
from models import User, Role

def add_admin_user(username, password):
    with app.app_context():
        admin_role = Role.query.filter_by(name='admin').first()
        if not admin_role:
            print("Роль 'admin' не найдена. Создаем...")
            admin_role = Role(name='admin')
            user_role = Role.query.filter_by(name='user').first()
            if not user_role:
                user_role = Role(name='user')
                db.session.add(user_role)
            db.session.add(admin_role)
            db.session.commit()
            print(f"Роль 'admin' создана с ID: {admin_role.id}")

        existing_user = User.query.filter_by(username=username).first()
        if existing_user:
            print(f"Пользователь '{username}' уже существует.")
            return

        hashed_password = bcrypt.generate_password_hash(password).decode('utf-8')
        new_admin = User(username=username, password_hash=hashed_password, role_id=admin_role.id)
        db.session.add(new_admin)
        db.session.commit()
        print(f"Администратор '{username}' успешно создан с ID: {new_admin.id} и ролью 'admin'.")

if __name__ == '__main__':
    admin_username = input("Введите имя пользователя для администратора: ")
    admin_password = input("Введите пароль для администратора: ")
    add_admin_user(admin_username, admin_password)
```

### `backend\database.py`

```python
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
```

### `backend\load_data.py`

```python
import csv
import os
import logging
from decimal import Decimal

from flask_bcrypt import Bcrypt
from database import db
from models import User, Role, Order, OrderItem, Review, Vehicle
from app import app


# Настройка логирования для скрипта
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - [LOAD_DATA] - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.propagate = False

bcrypt = Bcrypt(app)

BASE_DIR            = os.path.dirname(os.path.abspath(__file__))
USERS_CSV_PATH      = os.path.join(BASE_DIR, 'data', 'users.csv')
REVIEWS_CSV_PATH    = os.path.join(BASE_DIR, 'data', 'reviews.csv')
VEHICLES_CSV_PATH   = os.path.join(BASE_DIR, 'data', 'project_vehicle_dataset.csv')

def load_vehicles():
    """Загружает автомобили из CSV, если таблица пуста."""
    if Vehicle.query.first():
        logger.info("Vehicles table is not empty, skipping CSV load for vehicles.")
        return

    logger.info(f"Attempting to load vehicles from: {VEHICLES_CSV_PATH}")
    if not os.path.exists(VEHICLES_CSV_PATH):
        logger.warning(f"Vehicles CSV file not found at {VEHICLES_CSV_PATH}.")
        return

    vehicles_to_add = []
    with open(VEHICLES_CSV_PATH, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        required = [
            'vehicle_id', 'brand', 'body_type', 'year', 'gearbox',
            'drive', 'power_hp', 'seats', 'price_per_day_usd'
        ]
        for i, row in enumerate(reader, start=1):
            if not all(row.get(col, '').strip() for col in required):
                logger.warning(f"Row {i}: missing required fields, skipping.")
                continue
            try:
                vid = row['vehicle_id'].strip()
                model_val = row.get('model', '').strip() or None
                img = row.get('image', '').strip()
                img = None if not img or img.lower() == 'n/a' else img.strip()

                v = Vehicle(
                    vehicle_id=vid,
                    brand=row['brand'].strip(),
                    model=model_val,
                    body_type=row['body_type'].strip(),
                    year=int(row['year']),
                    gearbox=row['gearbox'].strip(),
                    drive=row['drive'].strip(),
                    power_hp=int(row['power_hp']),
                    seats=int(row['seats']),
                    price_per_day_usd=Decimal(row['price_per_day_usd']),
                    image_url=img
                )
                vehicles_to_add.append(v)
            except Exception as e:
                logger.error(f"Row {i}: error parsing vehicle: {e}", exc_info=False)

    if vehicles_to_add:
        db.session.bulk_save_objects(vehicles_to_add)
        db.session.commit()
        logger.info(f"✔ Loaded {len(vehicles_to_add)} vehicles.")
    else:
        logger.info("No vehicles loaded (table empty or all rows invalid).")

def load_users_and_roles():
    """Создаёт роли и загружает пользователей из CSV, если таблица пуста.
       Гарантирует, что хотя бы один админ есть всегда."""
    user_role  = Role.query.filter_by(name='user').first()
    admin_role = Role.query.filter_by(name='admin').first()
    if not user_role:
        user_role = Role(name='user')
        db.session.add(user_role)
        logger.info("Created 'user' role.")
    if not admin_role:
        admin_role = Role(name='admin')
        db.session.add(admin_role)
        logger.info("Created 'admin' role.")
    db.session.commit()

    if User.query.first():
        logger.info("Users table is not empty, skipping CSV load for users.")
    else:
        if os.path.exists(USERS_CSV_PATH):
            logger.info(f"Attempting to load users from: {USERS_CSV_PATH}")
            added = 0
            with open(USERS_CSV_PATH, newline='', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for i, row in enumerate(reader, start=1):
                    try:
                        uid   = int(row['id'])
                        name  = row['name'].strip()
                        pw    = row['password'].strip()
                        if not name or not pw:
                            logger.warning(f"Row {i}: empty name or password, skipping.")
                            continue
                        pw_hash = bcrypt.generate_password_hash(pw).decode('utf-8')
                        role_id = user_role.id
                        if row.get('role', '').strip().lower() == 'admin':
                            role_id = admin_role.id

                        u = User(id=uid, username=name, password_hash=pw_hash, role_id=role_id)
                        db.session.add(u)
                        added += 1
                    except Exception as e:
                        logger.error(f"Row {i}: error parsing user: {e}", exc_info=False)

            if added:
                db.session.commit()
                logger.info(f"✔ Loaded {added} users from CSV.")
            else:
                logger.info("No users loaded from CSV (all rows invalid).")
        else:
            logger.warning(f"Users CSV not found at {USERS_CSV_PATH}, skipping CSV load.")

    admin_exists = User.query.join(Role).filter(Role.name=='admin').first()
    if not admin_exists:
        logger.info("No admin users found, creating default admin (admin/admin).")
        pw = bcrypt.generate_password_hash('admin').decode('utf-8')
        default_admin = User(username='admin', password_hash=pw, role_id=admin_role.id)
        db.session.add(default_admin)
        db.session.commit()
        logger.info("✔ Default admin user created: login=admin, password=admin.")

def load_reviews_and_orders():
    """Загружает отзывы из CSV и создаёт для каждого отзыв фейковый заказ."""
    if Review.query.first():
        logger.info("Reviews table is not empty, skipping CSV load for reviews.")
        return
    if not Vehicle.query.first() or not User.query.first():
        logger.info("Cannot load reviews: vehicles or users table is empty.")
        return

    logger.info(f"Attempting to load reviews/orders from: {REVIEWS_CSV_PATH}")
    if not os.path.exists(REVIEWS_CSV_PATH):
        logger.warning(f"Reviews CSV file not found at {REVIEWS_CSV_PATH}.")
        return

    valid_vids  = {str(v.vehicle_id) for v in Vehicle.query.with_entities(Vehicle.vehicle_id)}
    valid_uids  = {u.id for u in User.query.with_entities(User.id)}

    rev_count = 0
    ord_count = 0
    with open(REVIEWS_CSV_PATH, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader, start=1):
            try:
                uid_str = row['user_id'].strip()
                vid     = row['vehicle_id'].strip()
                uid     = int(uid_str)
                rating  = float(row['rating'].strip())
                comment = row.get('comment', '').strip() or None

                if uid not in valid_uids:
                    logger.warning(f"Row {i}: user {uid} not found, skipping.")
                    continue
                if vid not in valid_vids:
                    logger.warning(f"Row {i}: vehicle '{vid}' not found, skipping.")
                    continue
                if Review.query.filter_by(user_id=uid, vehicle_id=vid).first():
                    continue

                order = Order(user_id=uid)
                db.session.add(order)
                db.session.flush()
                ord_count += 1

                veh = Vehicle.query.get(vid)
                price = veh.price_per_day_usd if veh else Decimal('0.00')
                oi = OrderItem(order_id=order.id, vehicle_id=vid, price=price)
                db.session.add(oi)

                rev = Review(
                    user_id=uid,
                    vehicle_id=vid,
                    order_id=order.id,
                    rating=int(rating*2)/2.0,
                    comment=comment
                )
                db.session.add(rev)
                rev_count += 1

            except Exception as e:
                logger.error(f"Row {i}: error processing review: {e}", exc_info=False)
                db.session.rollback()
                continue

    if rev_count or ord_count:
        db.session.commit()
        logger.info(f"✔ Loaded {rev_count} reviews and created {ord_count} orders.")
    else:
        logger.info("No reviews loaded (table empty or all rows invalid).")

if __name__ == "__main__":
    with app.app_context():
        logger.info("=== Data loading started ===")
        load_users_and_roles()
        load_vehicles()
        load_reviews_and_orders()
        logger.info("=== Data loading finished ===")

```

### `backend\manage.py`

```python
import os
from flask.cli import FlaskGroup
from flask_migrate import Migrate
from app import app 
from database import db 

migrate = Migrate(app, db)

cli = FlaskGroup(app)

if __name__ == "__main__":
    cli()
```

### `backend\models.py`

```python
from database import db
from datetime import datetime

class Role(db.Model):
    __tablename__ = 'roles'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(64), unique=True, nullable=False)
    users = db.relationship('User', backref='role', lazy=True)

class User(db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(64), unique=True, nullable=False)
    password_hash = db.Column(db.String(128), nullable=False)
    role_id = db.Column(db.Integer, db.ForeignKey('roles.id'), nullable=False)
    cart_items = db.relationship('CartItem', backref='user', lazy=True, cascade="all, delete-orphan")
    orders     = db.relationship('Order',    backref='user', lazy=True)
    reviews    = db.relationship('Review',   backref='user', lazy=True)

class Vehicle(db.Model):
    __tablename__ = 'vehicles'
    vehicle_id        = db.Column(db.String(36), primary_key=True)
    brand             = db.Column(db.String(64), nullable=False)
    model             = db.Column(db.String(128), nullable=True)
    body_type         = db.Column(db.String(128), nullable=False)
    year              = db.Column(db.Integer, nullable=False)
    gearbox           = db.Column(db.String(30), nullable=False)
    drive             = db.Column(db.String(10), nullable=False)
    power_hp          = db.Column(db.Integer, nullable=False)
    seats             = db.Column(db.Integer, nullable=False)
    price_per_day_usd = db.Column(db.Numeric(10, 2), nullable=False)
    image_url         = db.Column(db.String(2048), nullable=True)
    reviews           = db.relationship('Review', backref='vehicle', lazy=True)

class CartItem(db.Model):
    __tablename__ = 'cart_items'
    id         = db.Column(db.Integer, primary_key=True)
    user_id    = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    vehicle_id = db.Column(db.String(36), db.ForeignKey('vehicles.vehicle_id'), nullable=False)
    vehicle    = db.relationship('Vehicle', lazy='joined')
    
    __table_args__ = ( 
        db.UniqueConstraint('user_id', 'vehicle_id', name='uq_user_vehicle_cart'),
    )

class Order(db.Model):
    __tablename__ = 'orders'
    id          = db.Column(db.Integer, primary_key=True)
    user_id     = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    created_at  = db.Column(db.DateTime, default=datetime.utcnow)
    items       = db.relationship('OrderItem', backref='order', lazy=True, cascade="all, delete-orphan")
    reviews     = db.relationship('Review', backref='order', lazy=True)

class OrderItem(db.Model):
    __tablename__ = 'order_items'
    id         = db.Column(db.Integer, primary_key=True)
    order_id   = db.Column(db.Integer, db.ForeignKey('orders.id'), nullable=False)
    vehicle_id = db.Column(db.String(36), db.ForeignKey('vehicles.vehicle_id'), nullable=False)
    price      = db.Column(db.Numeric(10, 2), nullable=False)
    vehicle    = db.relationship('Vehicle', lazy='joined')

class Review(db.Model):
    __tablename__ = 'reviews'
    id         = db.Column(db.Integer, primary_key=True)
    user_id    = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    vehicle_id = db.Column(db.String(36), db.ForeignKey('vehicles.vehicle_id'), nullable=False)
    order_id   = db.Column(db.Integer, db.ForeignKey('orders.id'), nullable=False)
    rating     = db.Column(db.Integer, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    comment    = db.Column(db.Text, nullable=True)

    def __repr__(self):
        return f"<Review user:{self.user_id} vehicle:'{self.vehicle_id}' order:{self.order_id} rating:{self.rating}>"
```

### `backend\train_recommendation.py`

import pandas as pd
import numpy as np
import pickle
from sqlalchemy import create_engine
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Dataset, Reader, KNNBasic
from collections import defaultdict

DB_USER = "root"
DB_PASSWORD = "123"
DB_HOST = "localhost"
DB_PORT     = 3306
DB_NAME = "db_name"

USER_CF_MODEL_FILE = 'user_knn_model.pkl'
CONTENT_MODEL_FILE = 'content_model.pkl'
HYBRID_MODEL_FILE  = 'hybrid_model.pkl'

ALPHA       = 0.7
K_NEIGHBORS = 40
TOP_K       = 10

# Загрузка данных
def load_data():
    conn_str = (
        f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}"
        f"@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    )
    engine = create_engine(conn_str)

    # истории заказов
    df_orders = pd.read_sql(
        """
        SELECT
          o.user_id,
          oi.vehicle_id     AS car_id,
          o.created_at      AS order_date
        FROM orders AS o
        JOIN order_items AS oi
          ON o.id = oi.order_id;
        """,
        engine
    )

    # отзывы
    df_reviews = pd.read_sql(
        "SELECT user_id, vehicle_id AS car_id, rating FROM reviews;",
        engine
    )

    # характеристики авто
    df_cars = pd.read_sql(
        """
        SELECT
          vehicle_id        AS car_id,
          model,
          drive             AS drive_type,
          year,
          power_hp,
          seats
        FROM vehicles;
        """,
        engine
    )

    return df_orders, df_reviews, df_cars


# Предобработка
def prepare_interactions(df_orders, df_reviews):
    df = pd.merge(df_orders, df_reviews, on=['user_id','car_id'], how='left')
    df['rating'] = df['rating'].fillna(3.0)
    df['order_date'] = pd.to_datetime(df['order_date'])
    df.sort_values('order_date', inplace=True)
    return df

def train_test_split(df_int):
    train_list, test_list = [], []
    for uid, grp in df_int.groupby('user_id'):
        recs = grp.to_dict('records')
        if len(recs) <= 1:
            test_list.extend(recs)
        else:
            *train_entries, test_entry = recs
            train_list.extend(train_entries)
            test_list.append(test_entry)
    return pd.DataFrame(train_list), pd.DataFrame(test_list)


# Построение моделей
def build_cf_model(df_train):
    reader   = Reader(rating_scale=(1,5))
    data     = Dataset.load_from_df(df_train[['user_id','car_id','rating']], reader)
    trainset = data.build_full_trainset()
    algo     = KNNBasic(k=K_NEIGHBORS, sim_options={'name':'cosine','user_based':True})
    algo.fit(trainset)
    return algo

def build_content_model(df_cars):
    df_feat = df_cars.set_index('car_id')
    features = ['model','drive_type','year','power_hp','seats']
    df_feat = pd.get_dummies(df_feat[features], columns=['model','drive_type'], drop_first=False)
    scaler  = MinMaxScaler()
    df_feat[['year','power_hp','seats']] = scaler.fit_transform(df_feat[['year','power_hp','seats']])
    X       = df_feat.values
    sim     = cosine_similarity(X, X)
    return df_feat, sim

# История пользователей
def build_user_history(df_train):
    return df_train.groupby('user_id')['car_id'].apply(list).to_dict()


# Рекомендательные функции
def recommend_content(user_id, df_feat, sim, history, top_n=TOP_K):
    cars = df_feat.index.tolist()
    seen = history.get(user_id, [])
    if not seen:
        return []
    idxs = [cars.index(c) for c in seen if c in cars]
    scores = sim[:, idxs].mean(axis=1)
    candidates = [(cars[i], scores[i]) for i in range(len(cars)) if cars[i] not in seen]
    candidates.sort(key=lambda x: x[1], reverse=True)
    return candidates[:top_n]

def recommend_hybrid(user_id, cf_algo, df_feat, sim, history,
                     item_mean, popular, alpha=ALPHA, top_n=TOP_K):
    cars = df_feat.index.tolist()
    seen = set(history.get(user_id, []))

    cf_scores = {c: (cf_algo.predict(user_id, c).est if c not in seen else 0)
                 for c in cars}
    # content-based
    cb_scores = {}
    if seen:
        idxs = [cars.index(c) for c in seen if c in cars]
        base = sim[:, idxs].mean(axis=1) if idxs else np.zeros(len(cars))
        for i,c in enumerate(cars):
            if c not in seen:
                cb_scores[c] = base[i] * 5
    else:
        return popular[:top_n]

    # hybrid
    hybrid = {c: alpha*cf_scores.get(c,0) + (1-alpha)*cb_scores.get(c,0)
              for c in cf_scores if c not in seen}
    ranked = sorted(hybrid.items(), key=lambda x: x[1], reverse=True)
    return ranked[:top_n]

# Оценка качества
def evaluate(rec_func, df_test, history, name, top_n=TOP_K):
    relevant = defaultdict(set)
    for _,r in df_test.iterrows():
        relevant[r['user_id']].add(r['car_id'])

    ps, rs, aps = [],[],[]
    for uid, trues in relevant.items():
        recs = [c for c,_ in rec_func(uid)]
        if not recs: continue
        hits = sum(1 for c in recs if c in trues)
        ps.append(hits/top_n)
        rs.append(hits/len(trues))
        cum, ap = 0,0
        for i,c in enumerate(recs,1):
            if c in trues:
                cum += 1
                ap += cum/i
        aps.append(ap/len(trues))

    print(f"{name}: P@{top_n}={np.mean(ps):.3f}, "
          f"R@{top_n}={np.mean(rs):.3f}, MAP@{top_n}={np.mean(aps):.3f}")

# main
def main():
    df_orders, df_reviews, df_cars = load_data()
    df_int    = prepare_interactions(df_orders, df_reviews)
    df_train, df_test = train_test_split(df_int)

    cf_algo = build_cf_model(df_train)
    feat, sim = build_content_model(df_cars)
    history = build_user_history(df_train)

    item_mean = df_train.groupby('car_id')['rating'].mean().to_dict()
    popular   = sorted(item_mean.items(), key=lambda x: x[1], reverse=True)

    evaluate(lambda u: recommend_hybrid(u, cf_algo, feat, sim, history, item_mean, popular),
             df_test, history, "Hybrid")
    evaluate(lambda u: recommend_content(u, feat, sim, history),
             df_test, history, "Content-based")
    evaluate(lambda u: recommend_hybrid(u, cf_algo, feat, sim, history, item_mean, popular, alpha=1.0),
             df_test, history, "User-based CF")

    # сохраняем
    with open(USER_CF_MODEL_FILE, 'wb')    as f: pickle.dump(cf_algo, f)
    with open(CONTENT_MODEL_FILE, 'wb')    as f: pickle.dump({'features': feat, 'cars': feat.index.tolist()}, f)
    with open(HYBRID_MODEL_FILE, 'wb')     as f: pickle.dump({'cf': cf_algo,'features':feat,'sim':sim,'alpha':ALPHA,'popular':popular}, f)

    print("Models saved.")

if __name__ == '__main__':
    main()

### `backend\Recommendations.ipynb`

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os

MIN_RATING_GLOBAL = 1
MAX_RATING_GLOBAL = 5

def evaluate(y_true, y_pred):
    y_true_arr = np.array(y_true)
    y_pred_arr = np.array(y_pred)
    valid_indices = ~np.isnan(y_true_arr) & ~np.isnan(y_pred_arr)
    if not np.all(valid_indices):
        print(f"Warning: NaNs found during evaluation. Removing {np.sum(~valid_indices)} pairs.")
        y_true_arr = y_true_arr[valid_indices]
        y_pred_arr = y_pred_arr[valid_indices]

    if len(y_true_arr) == 0:
        return {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan, 'Accuracy (Rounded)': np.nan}
    
    rmse = np.sqrt(mean_squared_error(y_true_arr, y_pred_arr))
    mae  = mean_absolute_error(y_true_arr, y_pred_arr)
    r2   = r2_score(y_true_arr, y_pred_arr)

    y_true_rounded = np.round(y_true_arr).astype(int)
    y_pred_rounded = np.round(y_pred_arr).astype(int)
    
    current_min_rating = MIN_RATING_GLOBAL 
    current_max_rating = MAX_RATING_GLOBAL
    
    y_true_rounded = np.clip(y_true_rounded, current_min_rating, current_max_rating)
    y_pred_rounded = np.clip(y_pred_rounded, current_min_rating, current_max_rating)

    accuracy_rounded = accuracy_score(y_true_rounded, y_pred_rounded) if len(y_true_rounded) > 0 else np.nan

    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Accuracy (Rounded)': accuracy_rounded}

vehicles_csv_path = r"C:\Users\Kirill\Desktop\Диплом\backend\data\project_vehicle_dataset.csv"
reviews_csv_path = r"C:\Users\Kirill\Desktop\Диплом\backend\data\reviews.csv" 

vehicles_orig = pd.DataFrame()
reviews_orig = pd.DataFrame()

try:
    print(f"Попытка загрузки vehicles из CSV: {vehicles_csv_path}")
    vehicles_orig = pd.read_csv(vehicles_csv_path)
    vehicles_orig['vehicle_id'] = vehicles_orig['vehicle_id'].astype(str).str.strip()
    print(f"Vehicles загружены из CSV: {vehicles_orig.shape}")

    if reviews_csv_path and os.path.exists(reviews_csv_path):
        print(f"Попытка загрузки reviews из CSV: {reviews_csv_path}")
        reviews_orig = pd.read_csv(reviews_csv_path)
        reviews_orig['vehicle_id'] = reviews_orig['vehicle_id'].astype(str).str.strip()
        if 'user_id' in reviews_orig.columns:
             reviews_orig['user_id'] = reviews_orig['user_id'].astype(str).str.strip()
        print(f"Reviews загружены из CSV: {reviews_orig.shape}")
    else:
        print("CSV файл с отзывами не найден или не указан. Генерация dummy отзывов (обычная)...")
        if vehicles_orig.empty or 'vehicle_id' not in vehicles_orig.columns:
            raise ValueError("Vehicles_orig пуст или не содержит 'vehicle_id' для генерации dummy отзывов.")

        n_users_dummy, n_reviews_per_vehicle_dummy = 70, 5 
        n_reviews_total_dummy = max(500, len(vehicles_orig) * n_reviews_per_vehicle_dummy) 

        valid_vehicle_ids_for_reviews = vehicles_orig['vehicle_id'].unique()
        if len(valid_vehicle_ids_for_reviews) == 0:
            raise ValueError("В загруженных vehicles нет уникальных vehicle_id для генерации dummy отзывов.")

        reviews_orig = pd.DataFrame({
            'review_id': range(1, n_reviews_total_dummy + 1),
            'user_id': [str(uid) for uid in np.random.randint(1, n_users_dummy + 1, n_reviews_total_dummy)],
            'vehicle_id': np.random.choice(valid_vehicle_ids_for_reviews, n_reviews_total_dummy, replace=True),
            'rating': np.random.randint(1, 6, n_reviews_total_dummy) 
        })
        print(f"Сгенерированы dummy отзывы (обычные): {reviews_orig.shape}")
        if 'user_id' in reviews_orig.columns:
            reviews_orig['user_id'] = reviews_orig['user_id'].astype(str)

except Exception as e:
    print(f"Ошибка при загрузке данных из CSV: {e}")
    print("Создание УЛУЧШЕННЫХ dummy данных для vehicles и reviews для демонстрации...")
    n_users, n_vehicles, n_reviews = 70, 150, 2000

    good_brands_internal = ['Toyota_g', 'Honda_g', 'BMW_g']
    bad_brands_internal = ['OldBrand_b', 'CheapBrand_b'] 
    neutral_brands_internal = ['Ford_n', 'Audi_n', 'VW_n', 'Mercedes_n', 'Nissan_n']
    all_brands_internal = good_brands_internal + bad_brands_internal + neutral_brands_internal
    
    vehicles_list = []
    for i in range(1, n_vehicles + 1):
        internal_brand = np.random.choice(all_brands_internal)
        brand = internal_brand.split('_')[0]
        
        body_type = np.random.choice(['Sedan', 'SUV', 'Hatchback', 'Coupe', 'Minivan', 'Pickup'])
        
        base_price = np.random.uniform(40, 160)
        power = np.random.randint(90, 250)
        year = np.random.randint(2010, 2024)

        if internal_brand in good_brands_internal:
            base_price *= np.random.uniform(1.3, 1.7)
            power = int(power * np.random.uniform(1.2, 1.5))
        elif internal_brand in bad_brands_internal:
            base_price *= np.random.uniform(0.6, 0.9)
            power = int(power * np.random.uniform(0.7, 0.9))
        
        if body_type == 'SUV': base_price *= 1.15
        elif body_type == 'Coupe': base_price *= 1.25
        
        base_price *= (1 + (year - 2010) * 0.04)
        power = min(max(80, power), 500) 
        base_price = max(15, base_price) 
            
        vehicles_list.append({
            'vehicle_id': str(i), 'brand': brand, 'model': f'Model_{brand[:2].upper()}{i}',
            'body_type': body_type, 'gearbox': np.random.choice(['Automatic', 'Manual', 'CVT']),
            'drive': np.random.choice(['FWD', 'RWD', 'AWD']), 'year': year,
            'power_hp': power, 'seats': np.random.randint(2, 8),
            'price_per_day_usd': round(base_price, 2)
        })
    vehicles_orig = pd.DataFrame(vehicles_list)
    vehicle_ids_for_dummy_reviews = vehicles_orig['vehicle_id'].unique()

    temp_vehicles_df = vehicles_orig.copy()
    temp_vehicles_df['internal_brand_marker'] = temp_vehicles_df.apply(
        lambda row: next((b for b in all_brands_internal if b.startswith(row['brand'])), None), axis=1
    )

    reviews_list = []
    user_preferences = {} 
    for user_id_int in range(1, n_users + 1):
        prefs = {}
        if np.random.rand() < 0.4: prefs['fav_brand_type'] = np.random.choice(['good', 'neutral'])
        else: prefs['fav_brand_type'] = 'any'
        if np.random.rand() < 0.3: prefs['fav_body'] = np.random.choice(vehicles_orig['body_type'].unique())
        user_preferences[str(user_id_int)] = prefs
        
    for review_idx in range(1, n_reviews + 1):
        user_id = str(np.random.randint(1, n_users + 1))
        vehicle_id = np.random.choice(vehicle_ids_for_dummy_reviews)
        
        vehicle_info = temp_vehicles_df.loc[temp_vehicles_df['vehicle_id'] == vehicle_id].iloc[0]
        
        rating_offset = 0
        vehicle_internal_brand = vehicle_info['internal_brand_marker']

        if vehicle_internal_brand in good_brands_internal: rating_offset += np.random.uniform(0.8, 1.5)
        elif vehicle_internal_brand in bad_brands_internal: rating_offset -= np.random.uniform(0.8, 1.5)
        
        if vehicle_info['body_type'] == 'SUV': rating_offset += np.random.uniform(0.2, 0.5)
        if vehicle_info['year'] > 2018: rating_offset += np.random.uniform(0.1, 0.4)

        # Учитываем предпочтения пользователя
        user_prefs = user_preferences.get(user_id, {})
        if user_prefs.get('fav_brand_type') == 'good' and vehicle_internal_brand in good_brands_internal:
            rating_offset += np.random.uniform(0.5, 1.0)
        elif user_prefs.get('fav_brand_type') == 'neutral' and vehicle_internal_brand in neutral_brands_internal:
             rating_offset += np.random.uniform(0.2, 0.6)
        if user_prefs.get('fav_body') == vehicle_info['body_type']:
            rating_offset += np.random.uniform(0.3, 0.7)

        raw_rating = 3.0 + rating_offset + np.random.normal(0, 0.7)
        rating = int(np.clip(np.round(raw_rating), 1, 5)) # Округляем и клипаем
        
        reviews_list.append({'review_id': review_idx, 'user_id': user_id, 'vehicle_id': vehicle_id, 'rating': rating})
    reviews_orig = pd.DataFrame(reviews_list)
    print(f"Сгенерированы УЛУЧШЕННЫЕ dummy отзывы: {reviews_orig.shape}")


vehicles = vehicles_orig.copy()
reviews = reviews_orig.copy()

if 'user_id' in reviews.columns: reviews['user_id'] = reviews['user_id'].astype(str)
if 'vehicle_id' in reviews.columns: reviews['vehicle_id'] = reviews['vehicle_id'].astype(str)
if 'vehicle_id' in vehicles.columns: vehicles['vehicle_id'] = vehicles['vehicle_id'].astype(str)


print("\n--- Информация о данных (до фильтрации) ---")
print("Vehicles shape:", vehicles.shape)
print("Reviews shape:", reviews.shape)
if not vehicles.empty:
    print("\nПервые 5 строк vehicles:")
    print(vehicles.head())
    key_vehicle_cols = ['vehicle_id', 'brand', 'model', 'body_type', 'year', 'gearbox', 'drive', 'power_hp', 'seats', 'price_per_day_usd']
    for col in key_vehicle_cols:
        if col in vehicles.columns and vehicles[col].isnull().any():
            print(f"Warning: Обнаружены NaN в vehicles['{col}'].")
            if vehicles[col].dtype in [np.float64, np.int64] and col not in ['year', 'seats']:
                 vehicles[col] = vehicles[col].fillna(vehicles[col].median())
            elif vehicles[col].dtype == 'object' and col not in ['vehicle_id', 'model']:
                 mode_val = vehicles[col].mode()
                 vehicles[col] = vehicles[col].fillna(mode_val[0] if not mode_val.empty else 'Unknown')

if not reviews.empty:
    print("\nПервые 5 строк reviews:")
    print(reviews.head())
    if 'rating' in reviews.columns:
        print("\nСтатистика по рейтингам:")
        print(reviews['rating'].describe())
        plt.figure(figsize=(8, 5)); sns.histplot(reviews['rating'], bins=reviews['rating'].nunique(), kde=False, discrete=True)
        plt.title('Распределение рейтингов'); plt.xlabel('Рейтинг'); plt.ylabel('Количество')
        # plt.show() 

    if 'user_id' in reviews.columns and 'rating' in reviews.columns:
        user_rating_counts = reviews.groupby('user_id')['rating'].count()
        plt.figure(figsize=(10, 6)); sns.histplot(user_rating_counts, bins=30, kde=False)
        plt.title('Распределение количества оценок на пользователя'); plt.xlabel('Количество оценок'); plt.ylabel('Количество пользователей')
        if not user_rating_counts.empty: plt.xlim(0, user_rating_counts.quantile(0.99) if user_rating_counts.quantile(0.99) > 0 else 10)
        # plt.show() 

    if 'vehicle_id' in reviews.columns and 'rating' in reviews.columns:
        vehicle_rating_counts = reviews.groupby('vehicle_id')['rating'].count()
        plt.figure(figsize=(10, 6)); sns.histplot(vehicle_rating_counts, bins=30, kde=False)
        plt.title('Распределение количества оценок на автомобиль'); plt.xlabel('Количество оценок'); plt.ylabel('Количество автомобилей')
        if not vehicle_rating_counts.empty: plt.xlim(0, vehicle_rating_counts.quantile(0.99) if vehicle_rating_counts.quantile(0.99) > 0 else 10)
        # plt.show() 

if not reviews.empty and not vehicles.empty and \
   'vehicle_id' in reviews.columns and 'user_id' in reviews.columns and \
   'vehicle_id' in vehicles.columns:

    min_vehicle_ratings = 5 
    min_user_ratings = 5    

    print(f"\nНачальный размер reviews: {len(reviews)}")
    vehicle_counts = reviews['vehicle_id'].value_counts()
    popular_vehicles_ids = vehicle_counts[vehicle_counts >= min_vehicle_ratings].index
    reviews = reviews[reviews['vehicle_id'].isin(popular_vehicles_ids)]
    print(f"Reviews после фильтрации по популярности авто (min_ratings={min_vehicle_ratings}): {len(reviews)}")

    if not reviews.empty: 
        user_counts = reviews['user_id'].value_counts()
        active_users_ids = user_counts[user_counts >= min_user_ratings].index
        reviews = reviews[reviews['user_id'].isin(active_users_ids)]
        print(f"Reviews после фильтрации по активности пользователей (min_ratings={min_user_ratings}): {len(reviews)}")
    else:
        print("Reviews стали пустыми после фильтрации по популярности авто.")

    if not reviews.empty:
        remaining_vehicle_ids = reviews['vehicle_id'].unique()
        vehicles = vehicles[vehicles['vehicle_id'].isin(remaining_vehicle_ids)]
    else:
        print("Reviews пусты после фильтрации, vehicles также будут пустыми.")
        vehicles = pd.DataFrame(columns=vehicles_orig.columns)

    print(f"Размер vehicles после фильтрации: {len(vehicles)}")

    if reviews.empty or vehicles.empty:
        print("После фильтрации не осталось данных. Возврат к исходным данным (до фильтрации).")
        reviews = reviews_orig.copy()
        vehicles = vehicles_orig.copy()
        if 'user_id' in reviews.columns: reviews['user_id'] = reviews['user_id'].astype(str)
        if 'vehicle_id' in reviews.columns: reviews['vehicle_id'] = reviews['vehicle_id'].astype(str)
        if 'vehicle_id' in vehicles.columns: vehicles['vehicle_id'] = vehicles['vehicle_id'].astype(str)

if not reviews.empty and 'rating' in reviews.columns and reviews['rating'].notna().any():
    MIN_RATING_GLOBAL = int(reviews['rating'].min())
    MAX_RATING_GLOBAL = int(reviews['rating'].max())
else:
    MIN_RATING_GLOBAL, MAX_RATING_GLOBAL = 1, 5
print(f"Установлены MIN_RATING_GLOBAL={MIN_RATING_GLOBAL}, MAX_RATING_GLOBAL={MAX_RATING_GLOBAL}")


if not reviews.empty and all(col in reviews.columns for col in ['user_id', 'vehicle_id', 'rating']):
    df = reviews[['user_id','vehicle_id','rating']].copy()
    df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
    df.dropna(subset=['rating'], inplace=True)
else:
    df = pd.DataFrame(columns=['user_id','vehicle_id','rating'])


if not df.empty:
    can_stratify = False
    if 'user_id' in df.columns:
        user_value_counts = df['user_id'].value_counts()
        if not user_value_counts.empty and user_value_counts.min() >= 2: can_stratify = True
            
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42,
                                         stratify=df['user_id'] if can_stratify else None)
else:
    train_df, test_df = pd.DataFrame(columns=df.columns), pd.DataFrame(columns=df.columns)
print(f"Train_df: {train_df.shape}, Test_df: {test_df.shape}")


if not train_df.empty:
    global_mean_rating = train_df['rating'].mean()
    if pd.isna(global_mean_rating): global_mean_rating = (MIN_RATING_GLOBAL + MAX_RATING_GLOBAL) / 2.0

    train_matrix = train_df.pivot_table(index='user_id', columns='vehicle_id', values='rating')
    user_means   = train_matrix.mean(axis=1).fillna(global_mean_rating)
    train_centered = train_matrix.sub(user_means, axis=0).fillna(0)
else:
    global_mean_rating = (MIN_RATING_GLOBAL + MAX_RATING_GLOBAL) / 2.0
    train_matrix = pd.DataFrame()
    user_means = pd.Series(dtype=float)
    train_centered = pd.DataFrame()

user_knn = None
if not train_centered.empty and len(train_centered.index) > 1:
    k_user_cf = min(20, len(train_centered.index) -1 if len(train_centered.index) > 1 else 1) 
    if k_user_cf > 0 :
        user_knn = NearestNeighbors(n_neighbors=k_user_cf, metric='cosine', algorithm='brute')
        user_knn.fit(train_centered.values)
        print(f"User-based CF: k_user_cf={k_user_cf}, n_samples_fit_={user_knn.n_samples_fit_}")
    else:
        print(f"User-based CF: k_user_cf равен 0 (мало уникальных пользователей: {len(train_centered.index)}), модель не будет обучена.")
else:
    print(f"User-based CF: недостаточно данных для обучения (train_centered empty: {train_centered.empty}, users: {len(train_centered.index)}).")


def predict_user_based(u, i, user_knn_model, train_centered_df, train_matrix_df, user_means_series, global_mean):
    if user_knn_model is None or train_centered_df.empty or u not in train_centered_df.index or i not in train_centered_df.columns:
        return user_means_series.get(u, global_mean)

    u_idx = train_centered_df.index.get_loc(u)
    user_vector = train_centered_df.iloc[[u_idx]].values

    if not np.any(user_vector): 
         return user_means_series.get(u, global_mean)

    n_samples_in_fit = user_knn_model.n_samples_fit_
    n_neighbors_actual = min(user_knn_model.n_neighbors, n_samples_in_fit)


    if n_neighbors_actual == 0: 
        return user_means_series.get(u, global_mean)

    dists, neighs_indices = user_knn_model.kneighbors(user_vector, n_neighbors=n_neighbors_actual)
    
    sims = 1 - dists.flatten()
    neigh_user_ids = train_centered_df.index[neighs_indices.flatten()]
    
    if i not in train_matrix_df.columns: 
        return user_means_series.get(u, global_mean)

    neigh_ratings_for_item_i = train_matrix_df.reindex(neigh_user_ids).get(i, pd.Series(dtype=float)) 
    
    if neigh_ratings_for_item_i.isna().all():
         return user_means_series.get(u, global_mean)

    valid_ratings_mask = ~neigh_ratings_for_item_i.isna()

    if not valid_ratings_mask.any():
        return user_means_series.get(u, global_mean)

    sims_filtered = sims[valid_ratings_mask]
    neigh_ratings_filtered = neigh_ratings_for_item_i[valid_ratings_mask]
    neigh_user_ids_filtered = neigh_user_ids[valid_ratings_mask] 
    
    relevant_user_means = user_means_series.reindex(neigh_user_ids_filtered).fillna(global_mean)

    numerator = (sims_filtered * (neigh_ratings_filtered - relevant_user_means)).sum()
    denominator = np.abs(sims_filtered).sum()

    if denominator == 0:
        return user_means_series.get(u, global_mean)
    
    prediction = user_means_series.get(u, global_mean) + numerator / denominator
    return np.clip(prediction, MIN_RATING_GLOBAL, MAX_RATING_GLOBAL)


y_true_ub, y_pred_ub = [], []
if user_knn is not None and not train_centered.empty and not test_df.empty:
    for _, row in test_df.iterrows():
        y_true_ub.append(row['rating'])
        y_pred_ub.append(predict_user_based(row['user_id'], row['vehicle_id'], user_knn, train_centered, train_matrix, user_means, global_mean_rating))
    
    y_pred_ub = [global_mean_rating if pd.isna(p) else p for p in y_pred_ub] 
    metrics_ub = evaluate(y_true_ub, y_pred_ub)
else:
    metrics_ub = {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan, 'Accuracy (Rounded)': np.nan} 
    if not test_df.empty:
        y_pred_ub = [global_mean_rating] * len(test_df) 
        y_true_ub = list(test_df['rating'])
    else: 
        y_pred_ub = []
        y_true_ub = []
print("User-based CF:", metrics_ub)

categorical_features = ['brand','model','body_type','gearbox','drive']
numeric_features     = ['year','power_hp','seats', 'price_per_day_usd']
actual_categorical_features = [f for f in categorical_features if f in vehicles.columns]
actual_numeric_features = [f for f in numeric_features if f in vehicles.columns]

X_items_df = pd.DataFrame()
item_pipeline = None
X_items_transformed = None
categorical_features_cb = list(actual_categorical_features) 

if not reviews.empty and 'vehicle_id' in reviews.columns:
    all_vehicle_ids_in_reviews = reviews['vehicle_id'].unique()
    if not vehicles.empty and 'vehicle_id' in vehicles.columns:
        X_items_df = vehicles[vehicles['vehicle_id'].isin(all_vehicle_ids_in_reviews)].copy()
else:
    print("Content-based: reviews dataframe is empty or missing 'vehicle_id'. X_items_df will be empty.")

if not X_items_df.empty:
    X_items_df = X_items_df.set_index('vehicle_id')
    
    if 'model' in X_items_df.columns and 'model' in categorical_features_cb:
        top_n_models = 50  
        model_counts = X_items_df['model'].value_counts()
        if not model_counts.empty:
            top_models = model_counts.nlargest(min(top_n_models, len(model_counts))).index
            X_items_df['model_grouped'] = X_items_df['model'].apply(lambda x: x if x in top_models else 'Other_Model')
            categorical_features_cb = [f if f != 'model' else 'model_grouped' for f in categorical_features_cb]
        else: 
            X_items_df['model_grouped'] = 'Other_Model'
            categorical_features_cb = [f if f != 'model' else 'model_grouped' for f in categorical_features_cb]
    elif 'model_grouped' not in X_items_df.columns:
         X_items_df['model_grouped'] = 'Other_Model' 
         if 'model_grouped' not in categorical_features_cb : 
            categorical_features_cb = [f for f in categorical_features_cb if f != 'model']
            categorical_features_cb.append('model_grouped')

    final_cat_features = [f for f in categorical_features_cb if f in X_items_df.columns]
    final_num_features = [f for f in actual_numeric_features if f in X_items_df.columns]

    for col in final_cat_features:
        if X_items_df[col].isnull().any():
            mode_val = X_items_df[col].mode()
            X_items_df[col] = X_items_df[col].fillna(mode_val[0] if not mode_val.empty else "Unknown")
    for col in final_num_features:
        if X_items_df[col].isnull().any():
            X_items_df[col] = X_items_df[col].fillna(X_items_df[col].median())

    transformers_list = []
    if final_cat_features: 
        transformers_list.append(('cat', OneHotEncoder(handle_unknown='ignore'), final_cat_features)) 
    if final_num_features: 
         transformers_list.append(('num', StandardScaler(), final_num_features))

    if transformers_list: 
        preprocessor = ColumnTransformer(transformers=transformers_list, remainder='drop')
        item_pipeline = Pipeline([('prep', preprocessor)])
        features_for_transform_df = X_items_df[final_cat_features + final_num_features]
        
        if not features_for_transform_df.empty:
            try:
                X_items_transformed = item_pipeline.fit_transform(features_for_transform_df)
                print(f"Content-based: X_items_transformed shape: {X_items_transformed.shape if X_items_transformed is not None else 'None'}")
            except ValueError as e:
                print(f"Ошибка при трансформации признаков для Content-based: {e}")
                X_items_transformed = None 
        else:
            print("Content-based: Нет признаков для трансформации или DataFrame признаков пуст.")
            X_items_transformed = None
    else:
        print("Content-based: Нет категориальных или числовых признаков для препроцессора.")
        X_items_transformed = None
else:
    print("Content-based: X_items_df пуст, модель не будет обучена.")
    final_cat_features, final_num_features = [], [] 

item_knn = None
if X_items_transformed is not None and X_items_transformed.shape[0] > 0 and X_items_transformed.shape[1] > 0 : 
    k_item_cf = 1 
    if X_items_transformed.shape[0] > 1: 
      k_item_cf = min(20, X_items_transformed.shape[0]-1 if X_items_transformed.shape[0]>1 else 1) 
    
    if k_item_cf > 0:
        item_knn = NearestNeighbors(n_neighbors=k_item_cf, metric='cosine', algorithm='brute')
        item_knn.fit(X_items_transformed)
        print(f"Content-based CF: k_item_cf={k_item_cf}, n_samples_fit_={item_knn.n_samples_fit_}")
    else:
        print(f"Content-based CF: k_item_cf равен 0 (мало образцов: {X_items_transformed.shape[0]}), модель не будет обучена.")
else:
    shape_info = X_items_transformed.shape if X_items_transformed is not None else "None"
    print(f"Content-based CF: X_items_transformed пуст ({shape_info}), имеет 0 признаков или мало образцов, модель не будет обучена.")

user_rated_history = {} 
if not train_df.empty and 'user_id' in train_df.columns and 'vehicle_id' in train_df.columns and 'rating' in train_df.columns:
    user_rated_history = (
        train_df.groupby('user_id')[['vehicle_id', 'rating']]
        .apply(lambda g: dict(zip(g['vehicle_id'], g['rating'])))
        .to_dict()
    )

def predict_content_based(u, i, item_knn_model, X_items_indexed_df, X_items_transformed_matrix, user_history_dict, global_mean, user_means_series):
    if item_knn_model is None or X_items_transformed_matrix is None or X_items_indexed_df.empty or i not in X_items_indexed_df.index:
        return user_means_series.get(u, global_mean) 

    if u not in user_history_dict or not user_history_dict[u]: 
        return user_means_series.get(u, global_mean) 

    try:
        item_idx_in_transformed = X_items_indexed_df.index.get_loc(i)
    except KeyError: 
        return user_means_series.get(u, global_mean)

    if item_idx_in_transformed >= X_items_transformed_matrix.shape[0]:
        return user_means_series.get(u, global_mean)

    item_vector = X_items_transformed_matrix[item_idx_in_transformed]
    if item_vector.ndim == 1: 
        item_vector = item_vector.reshape(1, -1)
    
    n_samples_in_fit = item_knn_model.n_samples_fit_
    n_neighbors_actual = min(item_knn_model.n_neighbors, n_samples_in_fit)
    if n_neighbors_actual == 0:
        return user_means_series.get(u, global_mean) 

    dists, neighs_indices = item_knn_model.kneighbors(item_vector, n_neighbors=n_neighbors_actual)
    similar_item_ids = X_items_indexed_df.index[neighs_indices.flatten()] 
    
    user_ratings_for_similar_items = []
    user_specific_history = user_history_dict[u] 
    for sim_item_id in similar_item_ids:
        if sim_item_id == i: continue 
        if sim_item_id in user_specific_history:
            user_ratings_for_similar_items.append(user_specific_history[sim_item_id])
            
    if not user_ratings_for_similar_items: 
        all_user_ratings = list(user_specific_history.values())
        return np.mean(all_user_ratings) if all_user_ratings else user_means_series.get(u, global_mean)
    
    prediction = np.mean(user_ratings_for_similar_items)
    return np.clip(prediction, MIN_RATING_GLOBAL, MAX_RATING_GLOBAL)


y_true_cb, y_pred_cb = [], []
if item_knn is not None and X_items_transformed is not None and not X_items_df.empty and not test_df.empty:
    for _, row in test_df.iterrows():
        y_true_cb.append(row['rating'])
        y_pred_cb.append(predict_content_based(row['user_id'], row['vehicle_id'], item_knn, X_items_df, X_items_transformed, user_rated_history, global_mean_rating, user_means))
    
    y_pred_cb = [global_mean_rating if pd.isna(p) else p for p in y_pred_cb]
    metrics_cb = evaluate(y_true_cb, y_pred_cb)
else:
    metrics_cb = {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan, 'Accuracy (Rounded)': np.nan} 
    if not test_df.empty:
        y_pred_cb = [global_mean_rating] * len(test_df)
        y_true_cb = list(test_df['rating']) 
    else:
        y_pred_cb = []
        y_true_cb = []
print("Content-based CF:", metrics_cb)


print("\n--- Подбор alpha для гибридной модели ---")
alphas = np.arange(0, 1.01, 0.1)
hybrid_metrics_history = []
best_hybrid_metrics = {'RMSE': float('inf'), 'MAE': float('inf'), 'R2': -float('inf'), 'Accuracy (Rounded)': -float('inf')}
best_alpha = 0.5 

y_true_hybrid = y_true_ub 
if not y_true_hybrid: 
    print("Нет данных для оценки гибридной модели (test_df пуст или User-based не дал y_true).")
else:
    if len(y_pred_ub) != len(y_true_hybrid):
        print(f"Warning: Длина y_pred_ub ({len(y_pred_ub)}) != y_true_hybrid ({len(y_true_hybrid)}). Заполняем global_mean_rating.")
        y_pred_ub = [global_mean_rating] * len(y_true_hybrid)
    if len(y_pred_cb) != len(y_true_hybrid):
        print(f"Warning: Длина y_pred_cb ({len(y_pred_cb)}) != y_true_hybrid ({len(y_true_hybrid)}). Заполняем global_mean_rating.")
        y_pred_cb = [global_mean_rating] * len(y_true_hybrid)

    for alpha_val in alphas:
        y_pred_hybrid = []
        for idx in range(len(y_true_hybrid)):
            ub_pred_val = y_pred_ub[idx]
            cb_pred_val = y_pred_cb[idx]
            if pd.isna(ub_pred_val): ub_pred_val = global_mean_rating
            if pd.isna(cb_pred_val): cb_pred_val = global_mean_rating
            pred_h = alpha_val * ub_pred_val + (1 - alpha_val) * cb_pred_val
            y_pred_hybrid.append(np.clip(pred_h, MIN_RATING_GLOBAL, MAX_RATING_GLOBAL))
        
        current_metrics_hybrid = evaluate(y_true_hybrid, y_pred_hybrid)
        hybrid_metrics_history.append({'alpha': alpha_val, **current_metrics_hybrid})
        
        if 'RMSE' in current_metrics_hybrid and not pd.isna(current_metrics_hybrid['RMSE']):
             print(f"Alpha: {alpha_val:.1f}, RMSE: {current_metrics_hybrid['RMSE']:.4f}, MAE: {current_metrics_hybrid['MAE']:.4f}, R2: {current_metrics_hybrid['R2']:.4f}, Acc (rnd): {current_metrics_hybrid.get('Accuracy (Rounded)', np.nan):.4f}")
             if current_metrics_hybrid['RMSE'] < best_hybrid_metrics.get('RMSE', float('inf')): 
                best_hybrid_metrics = current_metrics_hybrid 
                best_alpha = alpha_val
        else:
            print(f"Alpha: {alpha_val:.1f}, Метрики: NaN")

    if pd.isna(best_hybrid_metrics.get('RMSE', np.nan)): 
        print("\nНе удалось найти валидные метрики для гибридной модели. Alpha не определен.")
        best_alpha = 0.5 
    else:
        print(f"\nЛучший alpha для гибридной модели: {best_alpha:.1f}")
        print(f"Лучшие метрики для гибридной модели (α={best_alpha:.1f}): {best_hybrid_metrics}")

    alpha_results_df = pd.DataFrame(hybrid_metrics_history)
    if not alpha_results_df.empty and 'RMSE' in alpha_results_df.columns:
        alpha_results_df_cleaned = alpha_results_df.dropna(subset=['RMSE', 'MAE', 'R2', 'Accuracy (Rounded)'], how='any') 
        if not alpha_results_df_cleaned.empty:
            plt.figure(figsize=(18, 5)) 
            plt.subplot(1, 3, 1); plt.plot(alpha_results_df_cleaned['alpha'], alpha_results_df_cleaned['RMSE'], marker='o', label='RMSE')
            plt.plot(alpha_results_df_cleaned['alpha'], alpha_results_df_cleaned['MAE'], marker='s', label='MAE')
            plt.xlabel('Alpha'); plt.ylabel('Error Value'); plt.title('RMSE & MAE vs. Alpha'); plt.legend(); plt.grid(True)
            plt.subplot(1, 3, 2); plt.plot(alpha_results_df_cleaned['alpha'], alpha_results_df_cleaned['R2'], marker='d', label='R2 Score', color='green')
            plt.xlabel('Alpha'); plt.ylabel('R2 Score'); plt.title('R2 Score vs. Alpha'); plt.legend(); plt.grid(True)
            plt.subplot(1, 3, 3); plt.plot(alpha_results_df_cleaned['alpha'], alpha_results_df_cleaned['Accuracy (Rounded)'], marker='*', label='Accuracy (Rounded)', color='purple')
            plt.xlabel('Alpha'); plt.ylabel('Accuracy (Rounded)'); plt.title('Accuracy (Rounded) vs. Alpha'); plt.legend(); plt.grid(True)
            plt.tight_layout(); plt.show() 
        else: print("Недостаточно валидных данных для построения графика Alpha vs Metrics.")
    else: print("Нет данных для визуализации Alpha vs Metrics.")


artifacts_to_save = {
    'global_mean_rating': global_mean_rating, 'MIN_RATING': MIN_RATING_GLOBAL, 
    'MAX_RATING': MAX_RATING_GLOBAL, 
    'best_alpha_hybrid': best_alpha if 'RMSE' in best_hybrid_metrics and not pd.isna(best_hybrid_metrics.get('RMSE')) else 0.5
}
if user_knn and not train_centered.empty:
    artifacts_to_save.update({
        'user_knn': user_knn, 'train_centered_index': train_centered.index.tolist(), 
        'train_centered_columns': train_centered.columns.tolist(), 'user_means': user_means, 
        'train_matrix_columns': train_matrix.columns.tolist() if not train_matrix.empty else [],
        'train_matrix_index': train_matrix.index.tolist() if not train_matrix.empty else []
    })
if item_knn and item_pipeline and X_items_transformed is not None and not X_items_df.empty:
    artifacts_to_save.update({
        'item_knn': item_knn, 'item_pipeline': item_pipeline, 
        'X_items_df_index': X_items_df.index.tolist(), 
        'X_items_df_used_cat_features': final_cat_features,
        'X_items_df_used_num_features': final_num_features,
        'user_rated_history': user_rated_history
    })

save_directory = r"C:\Users\Kirill\Desktop\Диплом\backend\models"
file_name = "recommendation_artifacts_csv.joblib"
full_save_path = os.path.join(save_directory, file_name)
os.makedirs(save_directory, exist_ok=True) 
try:
    joblib.dump(artifacts_to_save, full_save_path)
    print(f"\nАртефакты модели сохранены в {full_save_path}")
    print("Сохраненные артефакты:", list(artifacts_to_save.keys()))
except Exception as e: print(f"Ошибка при сохранении артефактов: {e}")


model_names = ['User-based CF', 'Content-based CF', f'Hybrid CF (α={best_alpha:.1f})']
rmse_values = [metrics_ub.get('RMSE', np.nan), metrics_cb.get('RMSE', np.nan), best_hybrid_metrics.get('RMSE', np.nan)]
mae_values  = [metrics_ub.get('MAE', np.nan), metrics_cb.get('MAE', np.nan), best_hybrid_metrics.get('MAE', np.nan)]
r2_values   = [metrics_ub.get('R2', np.nan), metrics_cb.get('R2', np.nan), best_hybrid_metrics.get('R2', np.nan)]
accuracy_values = [metrics_ub.get('Accuracy (Rounded)', np.nan), metrics_cb.get('Accuracy (Rounded)', np.nan), best_hybrid_metrics.get('Accuracy (Rounded)', np.nan)] 

valid_metric_indices = [
    i for i, rmse in enumerate(rmse_values) if
    all(pd.notna(val) for val in [rmse, mae_values[i], r2_values[i], accuracy_values[i]])
]

if valid_metric_indices:
    names = [model_names[i] for i in valid_metric_indices]
    rmses = [rmse_values[i] for i in valid_metric_indices]
    maes  = [mae_values[i]  for i in valid_metric_indices]
    r2s   = [r2_values[i]   for i in valid_metric_indices]
    accs  = [accuracy_values[i] for i in valid_metric_indices] 

    if names: 
        x = np.arange(len(names)); width = 0.20 
        fig, ax1 = plt.subplots(figsize=(16, 8)) 
        rects1 = ax1.bar(x - width*1.5, rmses, width, label='RMSE', color='cornflowerblue')
        rects2 = ax1.bar(x - width*0.5, maes, width, label='MAE', color='lightcoral')
        ax1.set_ylabel('Error Value (RMSE, MAE)'); ax1.set_title('Сравнение метрик моделей')
        ax1.set_xticks(x); ax1.set_xticklabels(names, rotation=15, ha="right")
        ax1.legend(loc='upper left'); ax1.grid(axis='y', linestyle='--', alpha=0.7)

        def autolabel(rects, ax_target, is_score=False):
            for rect in rects:
                height = rect.get_height()
                if pd.isna(height): continue 
                offset_points = 3; va = 'bottom'
                if is_score and height < 0: offset_points = -12; va = 'top'
                ax_target.annotate(f'{height:.3f}',
                             xy=(rect.get_x() + rect.get_width() / 2, height),
                             xytext=(0, offset_points), textcoords="offset points",
                             ha='center', va=va, fontsize=8)
        autolabel(rects1, ax1); autolabel(rects2, ax1)
        ax2 = ax1.twinx()
        rects3 = ax2.bar(x + width*0.5, r2s, width, label='R2 Score', color='mediumseagreen')
        rects4 = ax2.bar(x + width*1.5, accs, width, label='Accuracy (Rounded)', color='gold') 
        ax2.set_ylabel('Score (R2, Accuracy)')
        handles, labels = [], []
        for r_list in [rects3, rects4]:
            if r_list and len(r_list) > 0 : handles.append(r_list[0]); labels.append(r_list[0].get_label())
        if handles: ax2.legend(handles, labels, loc='upper right')
        all_scores = [s for s_list in [r2s, accs] for s in s_list if pd.notna(s)]
        if all_scores:
            min_s, max_s = min(all_scores), max(all_scores)
            ymin_ax2 = min(min_s - 0.1 * abs(min_s if min_s !=0 else 0.1) , -0.05 if min_s >=0 else min_s - 0.1)
            ymax_ax2 = max(max_s + 0.1 * abs(max_s if max_s !=0 else 0.1), 0.05  if max_s <=0 else max_s + 0.1)
            ymin_ax2 = max(ymin_ax2, -1.1); ymax_ax2 = min(ymax_ax2, 1.1)  
            if ymin_ax2 >= ymax_ax2: ymax_ax2 = ymin_ax2 + 0.2 if ymin_ax2 < 0.85 else 1.05
            ax2.set_ylim([ymin_ax2, ymax_ax2])
        autolabel(rects3, ax2, is_score=True); autolabel(rects4, ax2, is_score=True) 
        fig.tight_layout(); plt.show() 
    else: print("Нет валидных данных для построения итогового графика.")
else: print("Нет данных для построения итогового графика (все метрики NaN).")
```

```
### `backend\recommender.py`

```python
import os
import json
import logging

import numpy as np
import pandas as pd
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.getLogger('tensorflow').setLevel(logging.ERROR)


from tensorflow.keras import layers, regularizers, optimizers
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sqlalchemy import create_engine

DB_URL = os.environ.get(
    'DATABASE_URL',
    'mysql+pymysql://root:PisyaMorja123@localhost/db_name'
)

def load_and_split_ratings_with_features(test_size=0.2, random_state=42):
    engine = create_engine(DB_URL)
    try:
        df = pd.read_sql("SELECT user_id, vehicle_id, rating FROM reviews;", engine)
    except Exception as e:
        print(f"Ошибка подключения к базе данных или получения данных: {e}")
        return None, None, None, None

    if df.empty:
        print("Данные из базы данных не получены.")
        return None, None, None, None

    df['user_id']    = df['user_id'].astype(str)
    df['vehicle_id'] = df['vehicle_id'].astype(str)
    df['rating']     = df['rating'].astype(int)

    counts = df['rating'].value_counts()
    min_samples_for_stratify = 2
    unique_ratings_in_df = df['rating'].unique()
    can_stratify = all(counts.get(r, 0) >= min_samples_for_stratify for r in unique_ratings_in_df)
    
    strat = df['rating'] if can_stratify else None
    if not can_stratify:
        print(f"Предупреждение: Не все классы рейтинга в наборе данных имеют >= {min_samples_for_stratify} примеров. Стратификация может быть частичной или отключена.")

    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=strat
    )

    if train_df.empty or test_df.empty:
        print(f"Тренировочных примеров: {len(train_df)}, Тестовых примеров: {len(test_df)}")
        print("Недостаточно данных для обучения/тестирования после разделения. Выход.")
        return None, None, None, None

    global_average_rating = train_df['rating'].mean()
    if pd.isna(global_average_rating): 
        global_average_rating = 3.0 

    user_agg = train_df.groupby('user_id')['rating'].agg(['mean', 'count']).reset_index()
    user_agg.columns = ['user_id', 'user_avg_rating', 'user_rating_count']

    item_agg = train_df.groupby('vehicle_id')['rating'].agg(['mean', 'count']).reset_index()
    item_agg.columns = ['vehicle_id', 'item_avg_rating', 'item_rating_count']

    train_df = pd.merge(train_df, user_agg, on='user_id', how='left')
    train_df = pd.merge(train_df, item_agg, on='vehicle_id', how='left')

    test_df = pd.merge(test_df, user_agg, on='user_id', how='left')
    test_df = pd.merge(test_df, item_agg, on='vehicle_id', how='left')

    feature_cols_avg = ['user_avg_rating', 'item_avg_rating']
    for col in feature_cols_avg:
        train_df[col] = train_df[col].fillna(global_average_rating)
        test_df[col] = test_df[col].fillna(global_average_rating)

    feature_cols_count = ['user_rating_count', 'item_rating_count']
    for col in feature_cols_count:
        train_df[col] = train_df[col].fillna(0)
        test_df[col] = test_df[col].fillna(0)
        
    numerical_feature_names = feature_cols_avg + feature_cols_count
    
    norm_layer_stats = {}
    for feature_name in numerical_feature_names:
        mean = train_df[feature_name].mean()
        variance = train_df[feature_name].var()
        if pd.isna(mean): mean = 0.0 
        if pd.isna(variance) or variance == 0:
            variance = 1e-6 
        norm_layer_stats[feature_name] = {'mean': mean, 'variance': variance}

    return train_df, test_df, norm_layer_stats, global_average_rating


class RatingClassificationModel(tf.keras.Model):
    def __init__(self, user_lookup, item_lookup, norm_layers,
                 embedding_dim=64,      
                 hidden_units=[128, 64], 
                 dropout_rate=0.5,       
                 l2_reg=2e-4,            
                 num_classes=5):
        super().__init__()
        self.user_lookup = user_lookup
        self.item_lookup = item_lookup
        self.norm_layers = norm_layers 
        self.embedding_dim_param = embedding_dim
        self.hidden_units_param = hidden_units
        self.dropout_rate_param = dropout_rate
        self.l2_reg_param = l2_reg
        self.num_classes_param = num_classes

        vocab_u = user_lookup.vocabulary_size()
        vocab_i = item_lookup.vocabulary_size()

        self.user_emb = layers.Embedding(
            vocab_u,
            self.embedding_dim_param,
            embeddings_regularizer=regularizers.l2(self.l2_reg_param),
            name="user_embedding"
        )
        self.item_emb = layers.Embedding(
            vocab_i,
            self.embedding_dim_param,
            embeddings_regularizer=regularizers.l2(self.l2_reg_param),
            name="item_embedding"
        )

        self.concat = layers.Concatenate(name="concat_embeddings_features")
        
        self.dense_layers_list = []
        if self.hidden_units_param:
            self.dense_layers_list.append(layers.Dropout(self.dropout_rate_param, name="initial_dropout"))
        
        for i, units in enumerate(self.hidden_units_param):
            self.dense_layers_list.append(layers.Dense(
                units,
                kernel_regularizer=regularizers.l2(self.l2_reg_param),
                name=f"dense_{units}_{i}"
            ))
            self.dense_layers_list.append(layers.BatchNormalization(name=f"bn_{units}_{i}"))
            self.dense_layers_list.append(layers.Activation('relu', name=f"relu_{units}_{i}"))
            self.dense_layers_list.append(layers.Dropout(self.dropout_rate_param, name=f"dropout_{units}_{i}"))
        
        if not self.hidden_units_param :
             self.dense_layers_list.append(layers.Dropout(self.dropout_rate_param, name="concat_dropout_no_hidden"))

        self.out = layers.Dense(self.num_classes_param, activation='softmax', name="output")

    def call(self, inputs, training=False):
        user_indices = self.user_lookup(inputs['user_id'])
        item_indices = self.item_lookup(inputs['item_id'])

        user_embedding = self.user_emb(user_indices)
        item_embedding = self.item_emb(item_indices)

        norm_features_list = []
        for name, norm_layer_obj in self.norm_layers.items(): 
            feature_input_orig = inputs[name]
            
            condition = tf.equal(tf.rank(feature_input_orig), 1)
            
            def expand_dim_fn(inp):
                return tf.expand_dims(inp, axis=-1)
            
            def no_change_fn(inp):
                return inp

            feature_input_processed = tf.cond(
                condition,
                lambda: expand_dim_fn(feature_input_orig), 
                lambda: no_change_fn(feature_input_orig)  
            )
            norm_features_list.append(norm_layer_obj(feature_input_processed))
        
        concat_inputs = [user_embedding, item_embedding] + norm_features_list
        x = self.concat(concat_inputs)

        for layer_obj in self.dense_layers_list:
            if isinstance(layer_obj, (layers.Dropout, layers.BatchNormalization)):
                x = layer_obj(x, training=training)
            else:
                x = layer_obj(x)
        
        return self.out(x)
    
    def get_config_params(self):
        return {
            'embedding_dim': self.embedding_dim_param,
            'hidden_units': self.hidden_units_param,
            'dropout_rate': self.dropout_rate_param,
            'l2_reg': self.l2_reg_param,
            'num_classes': self.num_classes_param
        }

def train_and_save(epochs=150, batch_size=128):
    train_df, test_df, norm_layer_stats, global_avg_rating = load_and_split_ratings_with_features()
    
    if train_df is None or test_df is None:
        print("Не удалось загрузить данные с признаками. Выход.")
        return
    
    numerical_feature_names = ['user_avg_rating', 'user_rating_count', 'item_avg_rating', 'item_rating_count']

    all_users = sorted(pd.concat([train_df.user_id, test_df.user_id]).unique().tolist())
    all_items = sorted(pd.concat([train_df.vehicle_id, test_df.vehicle_id]).unique().tolist())

    user_lookup = layers.StringLookup(vocabulary=all_users, mask_token=None, name="user_id_lookup")
    item_lookup = layers.StringLookup(vocabulary=all_items, mask_token=None, name="vehicle_id_lookup")

    norm_layers = {}
    for feature_name in numerical_feature_names:
        norm_layer_obj = layers.Normalization(axis=-1, name=f"norm_{feature_name}") 
        feature_data = train_df[feature_name].values.astype(np.float32).reshape(-1, 1)
        if len(feature_data) > 0:
            norm_layer_obj.adapt(feature_data)
        else:
            print(f"Предупреждение: Нет данных для адаптации слоя нормализации для {feature_name}. Используются веса по умолчанию (среднее=0, дисперсия=1).")
            norm_layer_obj.set_weights([np.array([0.0]), np.array([1.0])]) 
        norm_layers[feature_name] = norm_layer_obj

    classes_original_ratings = np.arange(1, 6)
    y_train_original = train_df['rating'].to_numpy()
    class_weights_dict = None
    if len(y_train_original) > 0:
        y_train_for_weighting = y_train_original[np.isin(y_train_original, classes_original_ratings)]
        if len(y_train_for_weighting) > 0:
            present_classes_in_train = np.unique(y_train_for_weighting)
            if len(present_classes_in_train) > 0 :
                sorted_present_classes = np.sort(present_classes_in_train)
                cw = compute_class_weight('balanced', 
                                          classes=sorted_present_classes, 
                                          y=y_train_for_weighting)
                class_weights_dict = {label - 1: weight for label, weight in zip(sorted_present_classes, cw)}
                print(f"Рассчитанные веса классов (для индексов 0-4): {class_weights_dict}")
            else: print("Предупреждение: В y_train не найдены допустимые классы (1-5) для расчета весов классов.")
        else: print("Предупреждение: y_train_original (после фильтрации по 1-5) пуст. Веса классов не рассчитаны.")
    else: print("Предупреждение: y_train_original пуст. Веса классов не рассчитаны.")

    num_classes_param = 5 

    def df_to_dataset(df, shuffle):
        x_dict = {
            'user_id': df['user_id'].to_numpy(),
            'item_id': df['vehicle_id'].to_numpy(),
        }
        for fname in numerical_feature_names: 
            x_dict[fname] = df[fname].to_numpy().astype(np.float32)

        if df['rating'].empty:
            raise ValueError("DataFrame для рейтингов пуст в df_to_dataset.")
        y = tf.one_hot(df['rating'] - 1, depth=num_classes_param) 
        ds = tf.data.Dataset.from_tensor_slices((x_dict, y))
        if shuffle:
            ds = ds.shuffle(buffer_size=len(df))
        return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    try:
        train_ds = df_to_dataset(train_df, shuffle=True)
        val_ds   = df_to_dataset(test_df, shuffle=False)
    except ValueError as e:
        print(f"Ошибка при создании датасета: {e}")
        return

    embedding_dim_param = 64     
    hidden_units_param = [128, 64] 
    dropout_rate_param = 0.5     
    l2_reg_param = 2e-4

    model = RatingClassificationModel(
        user_lookup, item_lookup, norm_layers,
        embedding_dim=embedding_dim_param,
        hidden_units=hidden_units_param,
        dropout_rate=dropout_rate_param,
        l2_reg=l2_reg_param,
        num_classes=num_classes_param
    )
    
    try:
        sample_x_batch, _ = next(iter(train_ds)) 
        _ = model(sample_x_batch, training=False) 
        print("Модель успешно построена с использованием тестового батча.")
    except Exception as e:
        print(f"Ошибка во время начального вызова модели (построения): {e}")
        import traceback
        traceback.print_exc()
        return

    # Гиперпараметры оптимизатора и коллбэков
    optimizer = optimizers.AdamW(
        learning_rate=3e-4,   
        weight_decay=2e-4,    
        clipnorm=1.0          
    )

    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.3, 
        patience=7, 
        min_lr=1e-7, 
        verbose=1 
    )
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=20,    
        min_delta=0.001, 
        restore_best_weights=True,
        verbose=1 
    )

    print("Начало обучения модели (v10_baseline_moderate_reg)...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs, 
        class_weight=class_weights_dict if class_weights_dict else None,
        callbacks=[reduce_lr, early_stopping],
        verbose=1 
    )
    print("Обучение завершено.")

    loss, acc = model.evaluate(val_ds, verbose=0)
    print(f"=== Итоговая точность на тестовом наборе: {acc:.4f} ===")
    print(f"=== Итоговые потери на тестовом наборе: {loss:.4f} ===")

    save_dir = os.path.join(os.getcwd(), 'saved_rating_model_cls_v10_baseline_moderate_reg') 
    os.makedirs(save_dir, exist_ok=True)
    
    model.save_weights(os.path.join(save_dir, 'model.weights.h5'))

    with open(os.path.join(save_dir, 'user_lookup_vocab.json'), 'w', encoding='utf-8') as f:
        json.dump(user_lookup.get_vocabulary(), f, ensure_ascii=False)
    with open(os.path.join(save_dir, 'item_lookup_vocab.json'), 'w', encoding='utf-8') as f:
        json.dump(item_lookup.get_vocabulary(), f, ensure_ascii=False)

    model_actual_config_params = model.get_config_params()
    with open(os.path.join(save_dir, 'model_config_params.json'), 'w', encoding='utf-8') as f:
        json.dump(model_actual_config_params, f, ensure_ascii=False)

    with open(os.path.join(save_dir, 'norm_layer_stats.json'), 'w', encoding='utf-8') as f:
        json.dump(norm_layer_stats, f, ensure_ascii=False)
        
    with open(os.path.join(save_dir, 'global_average_rating.json'), 'w', encoding='utf-8') as f:
        json.dump({'global_average_rating': global_avg_rating}, f, ensure_ascii=False)

    print(f"Модель и артефакты сохранены в {save_dir}")

if __name__ == '__main__':
    train_and_save()
```

